{
  "best_metric": 0.800343930721283,
  "best_model_checkpoint": "./5class/results-distilbert/checkpoint-81250",
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 81250,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0024615384615384616,
      "grad_norm": 1.4136826992034912,
      "learning_rate": 2.4614374794880215e-07,
      "loss": 1.6119,
      "step": 100
    },
    {
      "epoch": 0.004923076923076923,
      "grad_norm": 1.5168397426605225,
      "learning_rate": 4.922874958976043e-07,
      "loss": 1.6089,
      "step": 200
    },
    {
      "epoch": 0.0073846153846153844,
      "grad_norm": 2.3066184520721436,
      "learning_rate": 7.384312438464064e-07,
      "loss": 1.6084,
      "step": 300
    },
    {
      "epoch": 0.009846153846153846,
      "grad_norm": 2.031346082687378,
      "learning_rate": 9.845749917952086e-07,
      "loss": 1.6005,
      "step": 400
    },
    {
      "epoch": 0.012307692307692308,
      "grad_norm": 1.770750880241394,
      "learning_rate": 1.2307187397440107e-06,
      "loss": 1.5904,
      "step": 500
    },
    {
      "epoch": 0.014769230769230769,
      "grad_norm": 2.157501220703125,
      "learning_rate": 1.4768624876928128e-06,
      "loss": 1.5753,
      "step": 600
    },
    {
      "epoch": 0.01723076923076923,
      "grad_norm": 3.9071779251098633,
      "learning_rate": 1.7230062356416147e-06,
      "loss": 1.5059,
      "step": 700
    },
    {
      "epoch": 0.019692307692307693,
      "grad_norm": 2.727559804916382,
      "learning_rate": 1.969149983590417e-06,
      "loss": 1.3934,
      "step": 800
    },
    {
      "epoch": 0.022153846153846152,
      "grad_norm": 5.489665985107422,
      "learning_rate": 2.212832294059731e-06,
      "loss": 1.3243,
      "step": 900
    },
    {
      "epoch": 0.024615384615384615,
      "grad_norm": 8.592991828918457,
      "learning_rate": 2.458976042008533e-06,
      "loss": 1.2359,
      "step": 1000
    },
    {
      "epoch": 0.02707692307692308,
      "grad_norm": 4.534502983093262,
      "learning_rate": 2.705119789957335e-06,
      "loss": 1.2181,
      "step": 1100
    },
    {
      "epoch": 0.029538461538461538,
      "grad_norm": 11.522612571716309,
      "learning_rate": 2.9488021004266493e-06,
      "loss": 1.1966,
      "step": 1200
    },
    {
      "epoch": 0.032,
      "grad_norm": 9.085811614990234,
      "learning_rate": 3.1949458483754516e-06,
      "loss": 1.1502,
      "step": 1300
    },
    {
      "epoch": 0.03446153846153846,
      "grad_norm": 5.788202285766602,
      "learning_rate": 3.4410895963242535e-06,
      "loss": 1.127,
      "step": 1400
    },
    {
      "epoch": 0.036923076923076927,
      "grad_norm": 8.371828079223633,
      "learning_rate": 3.6872333442730558e-06,
      "loss": 1.1229,
      "step": 1500
    },
    {
      "epoch": 0.039384615384615386,
      "grad_norm": 11.893139839172363,
      "learning_rate": 3.933377092221857e-06,
      "loss": 1.0863,
      "step": 1600
    },
    {
      "epoch": 0.041846153846153845,
      "grad_norm": 5.663896083831787,
      "learning_rate": 4.179520840170659e-06,
      "loss": 1.0656,
      "step": 1700
    },
    {
      "epoch": 0.044307692307692305,
      "grad_norm": 9.928041458129883,
      "learning_rate": 4.425664588119462e-06,
      "loss": 1.1159,
      "step": 1800
    },
    {
      "epoch": 0.04676923076923077,
      "grad_norm": 8.902952194213867,
      "learning_rate": 4.671808336068264e-06,
      "loss": 1.0792,
      "step": 1900
    },
    {
      "epoch": 0.04923076923076923,
      "grad_norm": 5.441401481628418,
      "learning_rate": 4.917952084017066e-06,
      "loss": 1.0408,
      "step": 2000
    },
    {
      "epoch": 0.05169230769230769,
      "grad_norm": 7.768742084503174,
      "learning_rate": 5.164095831965868e-06,
      "loss": 1.0741,
      "step": 2100
    },
    {
      "epoch": 0.05415384615384616,
      "grad_norm": 8.169053077697754,
      "learning_rate": 5.41023957991467e-06,
      "loss": 1.0214,
      "step": 2200
    },
    {
      "epoch": 0.056615384615384616,
      "grad_norm": 8.29866886138916,
      "learning_rate": 5.656383327863472e-06,
      "loss": 1.049,
      "step": 2300
    },
    {
      "epoch": 0.059076923076923075,
      "grad_norm": 9.28373908996582,
      "learning_rate": 5.902527075812274e-06,
      "loss": 1.0714,
      "step": 2400
    },
    {
      "epoch": 0.06153846153846154,
      "grad_norm": 9.344900131225586,
      "learning_rate": 6.148670823761077e-06,
      "loss": 1.0642,
      "step": 2500
    },
    {
      "epoch": 0.064,
      "grad_norm": 11.307062149047852,
      "learning_rate": 6.394814571709879e-06,
      "loss": 1.033,
      "step": 2600
    },
    {
      "epoch": 0.06646153846153846,
      "grad_norm": 10.581733703613281,
      "learning_rate": 6.6409583196586805e-06,
      "loss": 1.0285,
      "step": 2700
    },
    {
      "epoch": 0.06892307692307692,
      "grad_norm": 9.934412002563477,
      "learning_rate": 6.887102067607483e-06,
      "loss": 1.0138,
      "step": 2800
    },
    {
      "epoch": 0.07138461538461538,
      "grad_norm": 7.245753288269043,
      "learning_rate": 7.133245815556285e-06,
      "loss": 1.0042,
      "step": 2900
    },
    {
      "epoch": 0.07384615384615385,
      "grad_norm": 10.902924537658691,
      "learning_rate": 7.379389563505087e-06,
      "loss": 1.0037,
      "step": 3000
    },
    {
      "epoch": 0.07630769230769231,
      "grad_norm": 10.937490463256836,
      "learning_rate": 7.62553331145389e-06,
      "loss": 1.0295,
      "step": 3100
    },
    {
      "epoch": 0.07876923076923077,
      "grad_norm": 8.687084197998047,
      "learning_rate": 7.871677059402692e-06,
      "loss": 0.9865,
      "step": 3200
    },
    {
      "epoch": 0.08123076923076923,
      "grad_norm": 7.59515905380249,
      "learning_rate": 8.117820807351493e-06,
      "loss": 1.0039,
      "step": 3300
    },
    {
      "epoch": 0.08369230769230769,
      "grad_norm": 10.360139846801758,
      "learning_rate": 8.361503117820807e-06,
      "loss": 1.0109,
      "step": 3400
    },
    {
      "epoch": 0.08615384615384615,
      "grad_norm": 10.118866920471191,
      "learning_rate": 8.607646865769609e-06,
      "loss": 0.9936,
      "step": 3500
    },
    {
      "epoch": 0.08861538461538461,
      "grad_norm": 7.326611518859863,
      "learning_rate": 8.85379061371841e-06,
      "loss": 0.9631,
      "step": 3600
    },
    {
      "epoch": 0.09107692307692308,
      "grad_norm": 6.926167964935303,
      "learning_rate": 9.099934361667212e-06,
      "loss": 1.0553,
      "step": 3700
    },
    {
      "epoch": 0.09353846153846154,
      "grad_norm": 7.9254326820373535,
      "learning_rate": 9.346078109616016e-06,
      "loss": 0.9983,
      "step": 3800
    },
    {
      "epoch": 0.096,
      "grad_norm": 6.968517303466797,
      "learning_rate": 9.592221857564818e-06,
      "loss": 0.9857,
      "step": 3900
    },
    {
      "epoch": 0.09846153846153846,
      "grad_norm": 6.661440849304199,
      "learning_rate": 9.835904168034131e-06,
      "loss": 0.9883,
      "step": 4000
    },
    {
      "epoch": 0.10092307692307692,
      "grad_norm": 11.626144409179688,
      "learning_rate": 1.0082047915982933e-05,
      "loss": 1.0202,
      "step": 4100
    },
    {
      "epoch": 0.10338461538461538,
      "grad_norm": 7.162297248840332,
      "learning_rate": 1.0328191663931737e-05,
      "loss": 0.9725,
      "step": 4200
    },
    {
      "epoch": 0.10584615384615384,
      "grad_norm": 8.855779647827148,
      "learning_rate": 1.0574335411880539e-05,
      "loss": 1.0128,
      "step": 4300
    },
    {
      "epoch": 0.10830769230769231,
      "grad_norm": 9.891266822814941,
      "learning_rate": 1.082047915982934e-05,
      "loss": 0.9717,
      "step": 4400
    },
    {
      "epoch": 0.11076923076923077,
      "grad_norm": 5.192319869995117,
      "learning_rate": 1.1066622907778142e-05,
      "loss": 0.9883,
      "step": 4500
    },
    {
      "epoch": 0.11323076923076923,
      "grad_norm": 8.524802207946777,
      "learning_rate": 1.1312766655726944e-05,
      "loss": 0.9731,
      "step": 4600
    },
    {
      "epoch": 0.11569230769230769,
      "grad_norm": 6.879384994506836,
      "learning_rate": 1.1558910403675746e-05,
      "loss": 0.9655,
      "step": 4700
    },
    {
      "epoch": 0.11815384615384615,
      "grad_norm": 11.687394142150879,
      "learning_rate": 1.1805054151624548e-05,
      "loss": 0.9632,
      "step": 4800
    },
    {
      "epoch": 0.12061538461538461,
      "grad_norm": 9.33181381225586,
      "learning_rate": 1.2051197899573352e-05,
      "loss": 0.9813,
      "step": 4900
    },
    {
      "epoch": 0.12307692307692308,
      "grad_norm": 8.635629653930664,
      "learning_rate": 1.2297341647522153e-05,
      "loss": 0.9376,
      "step": 5000
    },
    {
      "epoch": 0.12553846153846154,
      "grad_norm": 4.700453281402588,
      "learning_rate": 1.2543485395470955e-05,
      "loss": 0.9531,
      "step": 5100
    },
    {
      "epoch": 0.128,
      "grad_norm": 9.345563888549805,
      "learning_rate": 1.2789629143419757e-05,
      "loss": 0.965,
      "step": 5200
    },
    {
      "epoch": 0.13046153846153846,
      "grad_norm": 7.676152229309082,
      "learning_rate": 1.3035772891368559e-05,
      "loss": 0.9675,
      "step": 5300
    },
    {
      "epoch": 0.13292307692307692,
      "grad_norm": 9.322480201721191,
      "learning_rate": 1.3281916639317361e-05,
      "loss": 0.9772,
      "step": 5400
    },
    {
      "epoch": 0.13538461538461538,
      "grad_norm": 8.831971168518066,
      "learning_rate": 1.3528060387266163e-05,
      "loss": 0.9911,
      "step": 5500
    },
    {
      "epoch": 0.13784615384615384,
      "grad_norm": 10.90584945678711,
      "learning_rate": 1.3774204135214966e-05,
      "loss": 0.9369,
      "step": 5600
    },
    {
      "epoch": 0.1403076923076923,
      "grad_norm": 17.512332916259766,
      "learning_rate": 1.4020347883163768e-05,
      "loss": 0.9568,
      "step": 5700
    },
    {
      "epoch": 0.14276923076923076,
      "grad_norm": 8.524435043334961,
      "learning_rate": 1.426649163111257e-05,
      "loss": 0.9419,
      "step": 5800
    },
    {
      "epoch": 0.14523076923076922,
      "grad_norm": 6.643252372741699,
      "learning_rate": 1.4512635379061372e-05,
      "loss": 0.9192,
      "step": 5900
    },
    {
      "epoch": 0.1476923076923077,
      "grad_norm": 8.456279754638672,
      "learning_rate": 1.4758779127010174e-05,
      "loss": 0.9525,
      "step": 6000
    },
    {
      "epoch": 0.15015384615384617,
      "grad_norm": 6.514505863189697,
      "learning_rate": 1.5004922874958976e-05,
      "loss": 0.9444,
      "step": 6100
    },
    {
      "epoch": 0.15261538461538462,
      "grad_norm": 5.3196282386779785,
      "learning_rate": 1.525106662290778e-05,
      "loss": 0.9499,
      "step": 6200
    },
    {
      "epoch": 0.15507692307692308,
      "grad_norm": 8.074846267700195,
      "learning_rate": 1.549721037085658e-05,
      "loss": 0.9244,
      "step": 6300
    },
    {
      "epoch": 0.15753846153846154,
      "grad_norm": 12.912429809570312,
      "learning_rate": 1.5743354118805383e-05,
      "loss": 0.9223,
      "step": 6400
    },
    {
      "epoch": 0.16,
      "grad_norm": 6.891655921936035,
      "learning_rate": 1.5989497866754183e-05,
      "loss": 0.944,
      "step": 6500
    },
    {
      "epoch": 0.16246153846153846,
      "grad_norm": 9.423815727233887,
      "learning_rate": 1.6235641614702987e-05,
      "loss": 0.9827,
      "step": 6600
    },
    {
      "epoch": 0.16492307692307692,
      "grad_norm": 4.851202487945557,
      "learning_rate": 1.648178536265179e-05,
      "loss": 0.9138,
      "step": 6700
    },
    {
      "epoch": 0.16738461538461538,
      "grad_norm": 6.750421524047852,
      "learning_rate": 1.6727929110600594e-05,
      "loss": 0.9362,
      "step": 6800
    },
    {
      "epoch": 0.16984615384615384,
      "grad_norm": 6.188493251800537,
      "learning_rate": 1.6974072858549394e-05,
      "loss": 0.943,
      "step": 6900
    },
    {
      "epoch": 0.1723076923076923,
      "grad_norm": 5.657182693481445,
      "learning_rate": 1.7220216606498198e-05,
      "loss": 0.9373,
      "step": 7000
    },
    {
      "epoch": 0.17476923076923076,
      "grad_norm": 9.35859489440918,
      "learning_rate": 1.7466360354446998e-05,
      "loss": 0.9416,
      "step": 7100
    },
    {
      "epoch": 0.17723076923076922,
      "grad_norm": 6.069173812866211,
      "learning_rate": 1.77125041023958e-05,
      "loss": 0.9283,
      "step": 7200
    },
    {
      "epoch": 0.17969230769230768,
      "grad_norm": 5.651230335235596,
      "learning_rate": 1.7958647850344602e-05,
      "loss": 0.953,
      "step": 7300
    },
    {
      "epoch": 0.18215384615384617,
      "grad_norm": 6.814886569976807,
      "learning_rate": 1.8204791598293405e-05,
      "loss": 0.9806,
      "step": 7400
    },
    {
      "epoch": 0.18461538461538463,
      "grad_norm": 11.247639656066895,
      "learning_rate": 1.8450935346242206e-05,
      "loss": 0.8817,
      "step": 7500
    },
    {
      "epoch": 0.18707692307692309,
      "grad_norm": 7.572787284851074,
      "learning_rate": 1.8697079094191006e-05,
      "loss": 0.9248,
      "step": 7600
    },
    {
      "epoch": 0.18953846153846154,
      "grad_norm": 7.8372578620910645,
      "learning_rate": 1.894322284213981e-05,
      "loss": 0.9026,
      "step": 7700
    },
    {
      "epoch": 0.192,
      "grad_norm": 4.778049468994141,
      "learning_rate": 1.918936659008861e-05,
      "loss": 0.8842,
      "step": 7800
    },
    {
      "epoch": 0.19446153846153846,
      "grad_norm": 9.522994041442871,
      "learning_rate": 1.9435510338037413e-05,
      "loss": 0.9604,
      "step": 7900
    },
    {
      "epoch": 0.19692307692307692,
      "grad_norm": 7.005018711090088,
      "learning_rate": 1.9681654085986213e-05,
      "loss": 0.9097,
      "step": 8000
    },
    {
      "epoch": 0.19938461538461538,
      "grad_norm": 6.895307540893555,
      "learning_rate": 1.992779783393502e-05,
      "loss": 0.9556,
      "step": 8100
    },
    {
      "epoch": 0.20184615384615384,
      "grad_norm": 8.616583824157715,
      "learning_rate": 2.017394158188382e-05,
      "loss": 0.937,
      "step": 8200
    },
    {
      "epoch": 0.2043076923076923,
      "grad_norm": 7.895412921905518,
      "learning_rate": 2.0420085329832624e-05,
      "loss": 0.9218,
      "step": 8300
    },
    {
      "epoch": 0.20676923076923076,
      "grad_norm": 7.094217300415039,
      "learning_rate": 2.0666229077781424e-05,
      "loss": 0.9204,
      "step": 8400
    },
    {
      "epoch": 0.20923076923076922,
      "grad_norm": 7.809116840362549,
      "learning_rate": 2.0912372825730228e-05,
      "loss": 0.961,
      "step": 8500
    },
    {
      "epoch": 0.21169230769230768,
      "grad_norm": 7.946142196655273,
      "learning_rate": 2.1158516573679028e-05,
      "loss": 0.9022,
      "step": 8600
    },
    {
      "epoch": 0.21415384615384617,
      "grad_norm": 7.10659646987915,
      "learning_rate": 2.140466032162783e-05,
      "loss": 0.9362,
      "step": 8700
    },
    {
      "epoch": 0.21661538461538463,
      "grad_norm": 5.673367023468018,
      "learning_rate": 2.1650804069576632e-05,
      "loss": 0.9398,
      "step": 8800
    },
    {
      "epoch": 0.21907692307692309,
      "grad_norm": 5.814817428588867,
      "learning_rate": 2.1896947817525435e-05,
      "loss": 0.9336,
      "step": 8900
    },
    {
      "epoch": 0.22153846153846155,
      "grad_norm": 7.865842342376709,
      "learning_rate": 2.2143091565474236e-05,
      "loss": 0.9527,
      "step": 9000
    },
    {
      "epoch": 0.224,
      "grad_norm": 5.649842262268066,
      "learning_rate": 2.238923531342304e-05,
      "loss": 0.9242,
      "step": 9100
    },
    {
      "epoch": 0.22646153846153846,
      "grad_norm": 5.534884929656982,
      "learning_rate": 2.263537906137184e-05,
      "loss": 0.919,
      "step": 9200
    },
    {
      "epoch": 0.22892307692307692,
      "grad_norm": 7.865711212158203,
      "learning_rate": 2.2881522809320643e-05,
      "loss": 0.914,
      "step": 9300
    },
    {
      "epoch": 0.23138461538461538,
      "grad_norm": 6.5428314208984375,
      "learning_rate": 2.3127666557269443e-05,
      "loss": 0.9468,
      "step": 9400
    },
    {
      "epoch": 0.23384615384615384,
      "grad_norm": 10.740447998046875,
      "learning_rate": 2.3373810305218247e-05,
      "loss": 0.9195,
      "step": 9500
    },
    {
      "epoch": 0.2363076923076923,
      "grad_norm": 7.239229679107666,
      "learning_rate": 2.361995405316705e-05,
      "loss": 0.8982,
      "step": 9600
    },
    {
      "epoch": 0.23876923076923076,
      "grad_norm": 8.44382381439209,
      "learning_rate": 2.3866097801115854e-05,
      "loss": 0.9172,
      "step": 9700
    },
    {
      "epoch": 0.24123076923076922,
      "grad_norm": 5.2924065589904785,
      "learning_rate": 2.4112241549064654e-05,
      "loss": 0.8887,
      "step": 9800
    },
    {
      "epoch": 0.24369230769230768,
      "grad_norm": 4.648550510406494,
      "learning_rate": 2.4358385297013458e-05,
      "loss": 0.9202,
      "step": 9900
    },
    {
      "epoch": 0.24615384615384617,
      "grad_norm": Infinity,
      "learning_rate": 2.4602067607482773e-05,
      "loss": 0.922,
      "step": 10000
    },
    {
      "epoch": 0.24861538461538463,
      "grad_norm": 8.641748428344727,
      "learning_rate": 2.4845749917952084e-05,
      "loss": 0.9093,
      "step": 10100
    },
    {
      "epoch": 0.2510769230769231,
      "grad_norm": 10.240875244140625,
      "learning_rate": 2.5091893665900884e-05,
      "loss": 0.9657,
      "step": 10200
    },
    {
      "epoch": 0.25353846153846155,
      "grad_norm": 7.334540367126465,
      "learning_rate": 2.5338037413849688e-05,
      "loss": 0.9223,
      "step": 10300
    },
    {
      "epoch": 0.256,
      "grad_norm": 7.124023914337158,
      "learning_rate": 2.558418116179849e-05,
      "loss": 0.8862,
      "step": 10400
    },
    {
      "epoch": 0.25846153846153846,
      "grad_norm": 6.818688869476318,
      "learning_rate": 2.5830324909747295e-05,
      "loss": 0.9259,
      "step": 10500
    },
    {
      "epoch": 0.2609230769230769,
      "grad_norm": 5.346022605895996,
      "learning_rate": 2.6076468657696095e-05,
      "loss": 0.9219,
      "step": 10600
    },
    {
      "epoch": 0.2633846153846154,
      "grad_norm": 9.37944221496582,
      "learning_rate": 2.63226124056449e-05,
      "loss": 0.9327,
      "step": 10700
    },
    {
      "epoch": 0.26584615384615384,
      "grad_norm": 6.9262847900390625,
      "learning_rate": 2.65687561535937e-05,
      "loss": 0.8837,
      "step": 10800
    },
    {
      "epoch": 0.2683076923076923,
      "grad_norm": 4.854450225830078,
      "learning_rate": 2.6814899901542503e-05,
      "loss": 0.9117,
      "step": 10900
    },
    {
      "epoch": 0.27076923076923076,
      "grad_norm": 4.704246997833252,
      "learning_rate": 2.7061043649491303e-05,
      "loss": 0.9182,
      "step": 11000
    },
    {
      "epoch": 0.2732307692307692,
      "grad_norm": 6.16623592376709,
      "learning_rate": 2.7307187397440106e-05,
      "loss": 0.9339,
      "step": 11100
    },
    {
      "epoch": 0.2756923076923077,
      "grad_norm": 6.482837200164795,
      "learning_rate": 2.7553331145388907e-05,
      "loss": 0.9077,
      "step": 11200
    },
    {
      "epoch": 0.27815384615384614,
      "grad_norm": 5.863094329833984,
      "learning_rate": 2.779947489333771e-05,
      "loss": 0.9027,
      "step": 11300
    },
    {
      "epoch": 0.2806153846153846,
      "grad_norm": 6.653873920440674,
      "learning_rate": 2.804561864128651e-05,
      "loss": 0.9296,
      "step": 11400
    },
    {
      "epoch": 0.28307692307692306,
      "grad_norm": 7.640618801116943,
      "learning_rate": 2.8291762389235314e-05,
      "loss": 0.8882,
      "step": 11500
    },
    {
      "epoch": 0.2855384615384615,
      "grad_norm": 4.9767327308654785,
      "learning_rate": 2.8537906137184114e-05,
      "loss": 0.8886,
      "step": 11600
    },
    {
      "epoch": 0.288,
      "grad_norm": 6.898773670196533,
      "learning_rate": 2.8784049885132918e-05,
      "loss": 0.9366,
      "step": 11700
    },
    {
      "epoch": 0.29046153846153844,
      "grad_norm": 10.875338554382324,
      "learning_rate": 2.903019363308172e-05,
      "loss": 0.8915,
      "step": 11800
    },
    {
      "epoch": 0.2929230769230769,
      "grad_norm": 4.598670959472656,
      "learning_rate": 2.9276337381030525e-05,
      "loss": 0.9155,
      "step": 11900
    },
    {
      "epoch": 0.2953846153846154,
      "grad_norm": 7.223084449768066,
      "learning_rate": 2.9522481128979325e-05,
      "loss": 0.9357,
      "step": 12000
    },
    {
      "epoch": 0.29784615384615387,
      "grad_norm": 3.335655450820923,
      "learning_rate": 2.976862487692813e-05,
      "loss": 0.887,
      "step": 12100
    },
    {
      "epoch": 0.30030769230769233,
      "grad_norm": 6.127307891845703,
      "learning_rate": 2.9998358966878483e-05,
      "loss": 0.9493,
      "step": 12200
    },
    {
      "epoch": 0.3027692307692308,
      "grad_norm": 8.38141918182373,
      "learning_rate": 2.9971008414853177e-05,
      "loss": 0.9254,
      "step": 12300
    },
    {
      "epoch": 0.30523076923076925,
      "grad_norm": 4.605752944946289,
      "learning_rate": 2.9943657862827867e-05,
      "loss": 0.8839,
      "step": 12400
    },
    {
      "epoch": 0.3076923076923077,
      "grad_norm": 7.180054187774658,
      "learning_rate": 2.9916307310802558e-05,
      "loss": 0.8999,
      "step": 12500
    },
    {
      "epoch": 0.31015384615384617,
      "grad_norm": 7.936158180236816,
      "learning_rate": 2.9888956758777248e-05,
      "loss": 0.9142,
      "step": 12600
    },
    {
      "epoch": 0.31261538461538463,
      "grad_norm": 5.907533645629883,
      "learning_rate": 2.986160620675194e-05,
      "loss": 0.9082,
      "step": 12700
    },
    {
      "epoch": 0.3150769230769231,
      "grad_norm": 6.7034993171691895,
      "learning_rate": 2.9834255654726633e-05,
      "loss": 0.913,
      "step": 12800
    },
    {
      "epoch": 0.31753846153846155,
      "grad_norm": 11.32140064239502,
      "learning_rate": 2.9806905102701323e-05,
      "loss": 0.8809,
      "step": 12900
    },
    {
      "epoch": 0.32,
      "grad_norm": 6.745604515075684,
      "learning_rate": 2.9779554550676014e-05,
      "loss": 0.8765,
      "step": 13000
    },
    {
      "epoch": 0.32246153846153847,
      "grad_norm": 6.51621675491333,
      "learning_rate": 2.9752203998650707e-05,
      "loss": 0.89,
      "step": 13100
    },
    {
      "epoch": 0.3249230769230769,
      "grad_norm": 7.200594425201416,
      "learning_rate": 2.9724853446625398e-05,
      "loss": 0.9264,
      "step": 13200
    },
    {
      "epoch": 0.3273846153846154,
      "grad_norm": 5.032736778259277,
      "learning_rate": 2.9697502894600092e-05,
      "loss": 0.8974,
      "step": 13300
    },
    {
      "epoch": 0.32984615384615384,
      "grad_norm": 6.334293842315674,
      "learning_rate": 2.9670152342574782e-05,
      "loss": 0.8911,
      "step": 13400
    },
    {
      "epoch": 0.3323076923076923,
      "grad_norm": 5.225469589233398,
      "learning_rate": 2.9642801790549473e-05,
      "loss": 0.9208,
      "step": 13500
    },
    {
      "epoch": 0.33476923076923076,
      "grad_norm": 5.628200054168701,
      "learning_rate": 2.9615451238524167e-05,
      "loss": 0.8488,
      "step": 13600
    },
    {
      "epoch": 0.3372307692307692,
      "grad_norm": 5.794277191162109,
      "learning_rate": 2.9588100686498857e-05,
      "loss": 0.9043,
      "step": 13700
    },
    {
      "epoch": 0.3396923076923077,
      "grad_norm": 4.358910083770752,
      "learning_rate": 2.956075013447355e-05,
      "loss": 0.8914,
      "step": 13800
    },
    {
      "epoch": 0.34215384615384614,
      "grad_norm": 4.137795925140381,
      "learning_rate": 2.953339958244824e-05,
      "loss": 0.8962,
      "step": 13900
    },
    {
      "epoch": 0.3446153846153846,
      "grad_norm": 6.531183242797852,
      "learning_rate": 2.950604903042293e-05,
      "loss": 0.8985,
      "step": 14000
    },
    {
      "epoch": 0.34707692307692306,
      "grad_norm": 3.7537145614624023,
      "learning_rate": 2.9478698478397623e-05,
      "loss": 0.9137,
      "step": 14100
    },
    {
      "epoch": 0.3495384615384615,
      "grad_norm": 7.391070365905762,
      "learning_rate": 2.945162143189257e-05,
      "loss": 0.9184,
      "step": 14200
    },
    {
      "epoch": 0.352,
      "grad_norm": 4.222808361053467,
      "learning_rate": 2.942427087986726e-05,
      "loss": 0.9152,
      "step": 14300
    },
    {
      "epoch": 0.35446153846153844,
      "grad_norm": 7.524852752685547,
      "learning_rate": 2.939692032784195e-05,
      "loss": 0.8942,
      "step": 14400
    },
    {
      "epoch": 0.3569230769230769,
      "grad_norm": 5.749605178833008,
      "learning_rate": 2.9369569775816642e-05,
      "loss": 0.9321,
      "step": 14500
    },
    {
      "epoch": 0.35938461538461536,
      "grad_norm": 4.585345268249512,
      "learning_rate": 2.9342219223791333e-05,
      "loss": 0.8668,
      "step": 14600
    },
    {
      "epoch": 0.3618461538461539,
      "grad_norm": 4.758202075958252,
      "learning_rate": 2.9314868671766027e-05,
      "loss": 0.9339,
      "step": 14700
    },
    {
      "epoch": 0.36430769230769233,
      "grad_norm": 6.889237880706787,
      "learning_rate": 2.9287518119740717e-05,
      "loss": 0.9057,
      "step": 14800
    },
    {
      "epoch": 0.3667692307692308,
      "grad_norm": 6.366583824157715,
      "learning_rate": 2.9260167567715408e-05,
      "loss": 0.8908,
      "step": 14900
    },
    {
      "epoch": 0.36923076923076925,
      "grad_norm": 7.837170124053955,
      "learning_rate": 2.9232817015690102e-05,
      "loss": 0.8486,
      "step": 15000
    },
    {
      "epoch": 0.3716923076923077,
      "grad_norm": 6.318699836730957,
      "learning_rate": 2.9205466463664792e-05,
      "loss": 0.8837,
      "step": 15100
    },
    {
      "epoch": 0.37415384615384617,
      "grad_norm": 8.027029991149902,
      "learning_rate": 2.9178115911639486e-05,
      "loss": 0.8619,
      "step": 15200
    },
    {
      "epoch": 0.37661538461538463,
      "grad_norm": 6.835721492767334,
      "learning_rate": 2.9150765359614177e-05,
      "loss": 0.9073,
      "step": 15300
    },
    {
      "epoch": 0.3790769230769231,
      "grad_norm": 9.39883041381836,
      "learning_rate": 2.9123414807588867e-05,
      "loss": 0.8657,
      "step": 15400
    },
    {
      "epoch": 0.38153846153846155,
      "grad_norm": 3.3337838649749756,
      "learning_rate": 2.909606425556356e-05,
      "loss": 0.8922,
      "step": 15500
    },
    {
      "epoch": 0.384,
      "grad_norm": 6.973710060119629,
      "learning_rate": 2.9068713703538248e-05,
      "loss": 0.8721,
      "step": 15600
    },
    {
      "epoch": 0.38646153846153847,
      "grad_norm": 6.90280294418335,
      "learning_rate": 2.9041363151512942e-05,
      "loss": 0.9006,
      "step": 15700
    },
    {
      "epoch": 0.3889230769230769,
      "grad_norm": 6.263881683349609,
      "learning_rate": 2.9014012599487633e-05,
      "loss": 0.8928,
      "step": 15800
    },
    {
      "epoch": 0.3913846153846154,
      "grad_norm": 5.045281410217285,
      "learning_rate": 2.8986662047462323e-05,
      "loss": 0.8941,
      "step": 15900
    },
    {
      "epoch": 0.39384615384615385,
      "grad_norm": 3.609447956085205,
      "learning_rate": 2.8959311495437017e-05,
      "loss": 0.9018,
      "step": 16000
    },
    {
      "epoch": 0.3963076923076923,
      "grad_norm": 13.056026458740234,
      "learning_rate": 2.8931960943411708e-05,
      "loss": 0.8916,
      "step": 16100
    },
    {
      "epoch": 0.39876923076923076,
      "grad_norm": 4.853707313537598,
      "learning_rate": 2.89046103913864e-05,
      "loss": 0.9039,
      "step": 16200
    },
    {
      "epoch": 0.4012307692307692,
      "grad_norm": 6.0503249168396,
      "learning_rate": 2.8877533344881343e-05,
      "loss": 0.8992,
      "step": 16300
    },
    {
      "epoch": 0.4036923076923077,
      "grad_norm": 8.876160621643066,
      "learning_rate": 2.8850182792856037e-05,
      "loss": 0.8935,
      "step": 16400
    },
    {
      "epoch": 0.40615384615384614,
      "grad_norm": 7.5303544998168945,
      "learning_rate": 2.8822832240830727e-05,
      "loss": 0.8718,
      "step": 16500
    },
    {
      "epoch": 0.4086153846153846,
      "grad_norm": 5.745389461517334,
      "learning_rate": 2.879548168880542e-05,
      "loss": 0.903,
      "step": 16600
    },
    {
      "epoch": 0.41107692307692306,
      "grad_norm": 7.608360290527344,
      "learning_rate": 2.876813113678011e-05,
      "loss": 0.8821,
      "step": 16700
    },
    {
      "epoch": 0.4135384615384615,
      "grad_norm": 8.903107643127441,
      "learning_rate": 2.8740780584754802e-05,
      "loss": 0.8567,
      "step": 16800
    },
    {
      "epoch": 0.416,
      "grad_norm": 5.496055603027344,
      "learning_rate": 2.8713430032729496e-05,
      "loss": 0.842,
      "step": 16900
    },
    {
      "epoch": 0.41846153846153844,
      "grad_norm": 8.316020011901855,
      "learning_rate": 2.8686079480704187e-05,
      "loss": 0.8851,
      "step": 17000
    },
    {
      "epoch": 0.4209230769230769,
      "grad_norm": 5.5731072425842285,
      "learning_rate": 2.865872892867888e-05,
      "loss": 0.8947,
      "step": 17100
    },
    {
      "epoch": 0.42338461538461536,
      "grad_norm": 8.141854286193848,
      "learning_rate": 2.863137837665357e-05,
      "loss": 0.9086,
      "step": 17200
    },
    {
      "epoch": 0.4258461538461538,
      "grad_norm": 6.682101726531982,
      "learning_rate": 2.860402782462826e-05,
      "loss": 0.9229,
      "step": 17300
    },
    {
      "epoch": 0.42830769230769233,
      "grad_norm": 5.703456878662109,
      "learning_rate": 2.8576677272602952e-05,
      "loss": 0.8442,
      "step": 17400
    },
    {
      "epoch": 0.4307692307692308,
      "grad_norm": 7.6311750411987305,
      "learning_rate": 2.8549326720577643e-05,
      "loss": 0.8522,
      "step": 17500
    },
    {
      "epoch": 0.43323076923076925,
      "grad_norm": 6.814839839935303,
      "learning_rate": 2.852224967407259e-05,
      "loss": 0.8516,
      "step": 17600
    },
    {
      "epoch": 0.4356923076923077,
      "grad_norm": 6.4463324546813965,
      "learning_rate": 2.849489912204728e-05,
      "loss": 0.8716,
      "step": 17700
    },
    {
      "epoch": 0.43815384615384617,
      "grad_norm": 6.261569499969482,
      "learning_rate": 2.8467548570021975e-05,
      "loss": 0.8839,
      "step": 17800
    },
    {
      "epoch": 0.44061538461538463,
      "grad_norm": 4.621135234832764,
      "learning_rate": 2.8440198017996662e-05,
      "loss": 0.8839,
      "step": 17900
    },
    {
      "epoch": 0.4430769230769231,
      "grad_norm": 5.734775066375732,
      "learning_rate": 2.8412847465971356e-05,
      "loss": 0.8717,
      "step": 18000
    },
    {
      "epoch": 0.44553846153846155,
      "grad_norm": 4.836266994476318,
      "learning_rate": 2.8385496913946047e-05,
      "loss": 0.8491,
      "step": 18100
    },
    {
      "epoch": 0.448,
      "grad_norm": 5.825295448303223,
      "learning_rate": 2.8358146361920737e-05,
      "loss": 0.8761,
      "step": 18200
    },
    {
      "epoch": 0.45046153846153847,
      "grad_norm": 5.933934688568115,
      "learning_rate": 2.833079580989543e-05,
      "loss": 0.8503,
      "step": 18300
    },
    {
      "epoch": 0.45292307692307693,
      "grad_norm": 10.498366355895996,
      "learning_rate": 2.830344525787012e-05,
      "loss": 0.9299,
      "step": 18400
    },
    {
      "epoch": 0.4553846153846154,
      "grad_norm": 4.803164482116699,
      "learning_rate": 2.8276094705844815e-05,
      "loss": 0.8618,
      "step": 18500
    },
    {
      "epoch": 0.45784615384615385,
      "grad_norm": 7.05524206161499,
      "learning_rate": 2.8248744153819506e-05,
      "loss": 0.8727,
      "step": 18600
    },
    {
      "epoch": 0.4603076923076923,
      "grad_norm": 8.591511726379395,
      "learning_rate": 2.8221393601794196e-05,
      "loss": 0.8664,
      "step": 18700
    },
    {
      "epoch": 0.46276923076923077,
      "grad_norm": 8.401018142700195,
      "learning_rate": 2.819404304976889e-05,
      "loss": 0.8824,
      "step": 18800
    },
    {
      "epoch": 0.4652307692307692,
      "grad_norm": 6.421034336090088,
      "learning_rate": 2.816669249774358e-05,
      "loss": 0.8732,
      "step": 18900
    },
    {
      "epoch": 0.4676923076923077,
      "grad_norm": 11.171662330627441,
      "learning_rate": 2.8139341945718275e-05,
      "loss": 0.8847,
      "step": 19000
    },
    {
      "epoch": 0.47015384615384614,
      "grad_norm": 5.658069133758545,
      "learning_rate": 2.8111991393692962e-05,
      "loss": 0.8717,
      "step": 19100
    },
    {
      "epoch": 0.4726153846153846,
      "grad_norm": 4.571942329406738,
      "learning_rate": 2.8084640841667652e-05,
      "loss": 0.8618,
      "step": 19200
    },
    {
      "epoch": 0.47507692307692306,
      "grad_norm": 7.742109298706055,
      "learning_rate": 2.8057290289642346e-05,
      "loss": 0.8575,
      "step": 19300
    },
    {
      "epoch": 0.4775384615384615,
      "grad_norm": 9.188996315002441,
      "learning_rate": 2.8029939737617037e-05,
      "loss": 0.8576,
      "step": 19400
    },
    {
      "epoch": 0.48,
      "grad_norm": 4.221425533294678,
      "learning_rate": 2.800258918559173e-05,
      "loss": 0.8485,
      "step": 19500
    },
    {
      "epoch": 0.48246153846153844,
      "grad_norm": 6.436062812805176,
      "learning_rate": 2.797523863356642e-05,
      "loss": 0.8773,
      "step": 19600
    },
    {
      "epoch": 0.4849230769230769,
      "grad_norm": 5.228151798248291,
      "learning_rate": 2.7947888081541112e-05,
      "loss": 0.8737,
      "step": 19700
    },
    {
      "epoch": 0.48738461538461536,
      "grad_norm": 8.206364631652832,
      "learning_rate": 2.7920537529515806e-05,
      "loss": 0.8463,
      "step": 19800
    },
    {
      "epoch": 0.4898461538461538,
      "grad_norm": 3.3534722328186035,
      "learning_rate": 2.7893186977490496e-05,
      "loss": 0.8933,
      "step": 19900
    },
    {
      "epoch": 0.49230769230769234,
      "grad_norm": 7.359912395477295,
      "learning_rate": 2.786583642546519e-05,
      "loss": 0.8673,
      "step": 20000
    },
    {
      "epoch": 0.4947692307692308,
      "grad_norm": 5.888956069946289,
      "learning_rate": 2.783848587343988e-05,
      "loss": 0.8641,
      "step": 20100
    },
    {
      "epoch": 0.49723076923076925,
      "grad_norm": 9.182588577270508,
      "learning_rate": 2.781113532141457e-05,
      "loss": 0.8594,
      "step": 20200
    },
    {
      "epoch": 0.4996923076923077,
      "grad_norm": 6.451655387878418,
      "learning_rate": 2.7783784769389265e-05,
      "loss": 0.8403,
      "step": 20300
    },
    {
      "epoch": 0.5021538461538462,
      "grad_norm": 6.219296455383301,
      "learning_rate": 2.7756434217363952e-05,
      "loss": 0.8829,
      "step": 20400
    },
    {
      "epoch": 0.5046153846153846,
      "grad_norm": 5.330272197723389,
      "learning_rate": 2.7729083665338646e-05,
      "loss": 0.8852,
      "step": 20500
    },
    {
      "epoch": 0.5070769230769231,
      "grad_norm": 4.407760143280029,
      "learning_rate": 2.7701733113313337e-05,
      "loss": 0.8636,
      "step": 20600
    },
    {
      "epoch": 0.5095384615384615,
      "grad_norm": 4.477354526519775,
      "learning_rate": 2.7674382561288027e-05,
      "loss": 0.8626,
      "step": 20700
    },
    {
      "epoch": 0.512,
      "grad_norm": 5.801409721374512,
      "learning_rate": 2.764703200926272e-05,
      "loss": 0.8503,
      "step": 20800
    },
    {
      "epoch": 0.5144615384615384,
      "grad_norm": 6.668246746063232,
      "learning_rate": 2.761968145723741e-05,
      "loss": 0.8447,
      "step": 20900
    },
    {
      "epoch": 0.5169230769230769,
      "grad_norm": 5.369814395904541,
      "learning_rate": 2.7592330905212105e-05,
      "loss": 0.862,
      "step": 21000
    },
    {
      "epoch": 0.5193846153846153,
      "grad_norm": 5.3495893478393555,
      "learning_rate": 2.7564980353186796e-05,
      "loss": 0.8723,
      "step": 21100
    },
    {
      "epoch": 0.5218461538461538,
      "grad_norm": 5.113633155822754,
      "learning_rate": 2.753790330668174e-05,
      "loss": 0.8667,
      "step": 21200
    },
    {
      "epoch": 0.5243076923076923,
      "grad_norm": 4.846841812133789,
      "learning_rate": 2.751055275465643e-05,
      "loss": 0.8357,
      "step": 21300
    },
    {
      "epoch": 0.5267692307692308,
      "grad_norm": 6.138879299163818,
      "learning_rate": 2.7483202202631125e-05,
      "loss": 0.8516,
      "step": 21400
    },
    {
      "epoch": 0.5292307692307693,
      "grad_norm": 8.713483810424805,
      "learning_rate": 2.7455851650605815e-05,
      "loss": 0.8661,
      "step": 21500
    },
    {
      "epoch": 0.5316923076923077,
      "grad_norm": 4.6804351806640625,
      "learning_rate": 2.7428501098580506e-05,
      "loss": 0.8427,
      "step": 21600
    },
    {
      "epoch": 0.5341538461538462,
      "grad_norm": 9.41617488861084,
      "learning_rate": 2.74011505465552e-05,
      "loss": 0.8574,
      "step": 21700
    },
    {
      "epoch": 0.5366153846153846,
      "grad_norm": 5.466375350952148,
      "learning_rate": 2.737379999452989e-05,
      "loss": 0.8843,
      "step": 21800
    },
    {
      "epoch": 0.5390769230769231,
      "grad_norm": 6.332231044769287,
      "learning_rate": 2.7346449442504584e-05,
      "loss": 0.8388,
      "step": 21900
    },
    {
      "epoch": 0.5415384615384615,
      "grad_norm": 5.307481288909912,
      "learning_rate": 2.7319098890479275e-05,
      "loss": 0.8206,
      "step": 22000
    },
    {
      "epoch": 0.544,
      "grad_norm": 3.8799290657043457,
      "learning_rate": 2.7291748338453965e-05,
      "loss": 0.8777,
      "step": 22100
    },
    {
      "epoch": 0.5464615384615384,
      "grad_norm": 3.95406174659729,
      "learning_rate": 2.7264397786428656e-05,
      "loss": 0.9109,
      "step": 22200
    },
    {
      "epoch": 0.548923076923077,
      "grad_norm": 5.651858806610107,
      "learning_rate": 2.7237047234403346e-05,
      "loss": 0.8495,
      "step": 22300
    },
    {
      "epoch": 0.5513846153846154,
      "grad_norm": 7.04656982421875,
      "learning_rate": 2.720969668237804e-05,
      "loss": 0.8521,
      "step": 22400
    },
    {
      "epoch": 0.5538461538461539,
      "grad_norm": 5.675081253051758,
      "learning_rate": 2.718234613035273e-05,
      "loss": 0.8618,
      "step": 22500
    },
    {
      "epoch": 0.5563076923076923,
      "grad_norm": 7.132175922393799,
      "learning_rate": 2.715499557832742e-05,
      "loss": 0.8724,
      "step": 22600
    },
    {
      "epoch": 0.5587692307692308,
      "grad_norm": 6.625155925750732,
      "learning_rate": 2.7127645026302115e-05,
      "loss": 0.8501,
      "step": 22700
    },
    {
      "epoch": 0.5612307692307692,
      "grad_norm": 2.855879068374634,
      "learning_rate": 2.7100294474276806e-05,
      "loss": 0.8658,
      "step": 22800
    },
    {
      "epoch": 0.5636923076923077,
      "grad_norm": 5.335093975067139,
      "learning_rate": 2.70729439222515e-05,
      "loss": 0.866,
      "step": 22900
    },
    {
      "epoch": 0.5661538461538461,
      "grad_norm": 4.187108039855957,
      "learning_rate": 2.704559337022619e-05,
      "loss": 0.8335,
      "step": 23000
    },
    {
      "epoch": 0.5686153846153846,
      "grad_norm": 3.127366542816162,
      "learning_rate": 2.701824281820088e-05,
      "loss": 0.8778,
      "step": 23100
    },
    {
      "epoch": 0.571076923076923,
      "grad_norm": 5.118619918823242,
      "learning_rate": 2.6990892266175575e-05,
      "loss": 0.8372,
      "step": 23200
    },
    {
      "epoch": 0.5735384615384616,
      "grad_norm": 6.075172424316406,
      "learning_rate": 2.6963541714150265e-05,
      "loss": 0.8358,
      "step": 23300
    },
    {
      "epoch": 0.576,
      "grad_norm": 10.3238525390625,
      "learning_rate": 2.6936191162124956e-05,
      "loss": 0.865,
      "step": 23400
    },
    {
      "epoch": 0.5784615384615385,
      "grad_norm": 6.543815612792969,
      "learning_rate": 2.6908840610099646e-05,
      "loss": 0.8834,
      "step": 23500
    },
    {
      "epoch": 0.5809230769230769,
      "grad_norm": 5.139257431030273,
      "learning_rate": 2.688149005807434e-05,
      "loss": 0.8518,
      "step": 23600
    },
    {
      "epoch": 0.5833846153846154,
      "grad_norm": 5.295337200164795,
      "learning_rate": 2.685413950604903e-05,
      "loss": 0.8509,
      "step": 23700
    },
    {
      "epoch": 0.5858461538461538,
      "grad_norm": 3.417261838912964,
      "learning_rate": 2.682678895402372e-05,
      "loss": 0.8643,
      "step": 23800
    },
    {
      "epoch": 0.5883076923076923,
      "grad_norm": 5.081725120544434,
      "learning_rate": 2.6799438401998415e-05,
      "loss": 0.8686,
      "step": 23900
    },
    {
      "epoch": 0.5907692307692308,
      "grad_norm": 5.847050189971924,
      "learning_rate": 2.6772087849973105e-05,
      "loss": 0.8366,
      "step": 24000
    },
    {
      "epoch": 0.5932307692307692,
      "grad_norm": 3.8390824794769287,
      "learning_rate": 2.67447372979478e-05,
      "loss": 0.8451,
      "step": 24100
    },
    {
      "epoch": 0.5956923076923077,
      "grad_norm": 7.584344863891602,
      "learning_rate": 2.671738674592249e-05,
      "loss": 0.8429,
      "step": 24200
    },
    {
      "epoch": 0.5981538461538461,
      "grad_norm": 4.309104919433594,
      "learning_rate": 2.669003619389718e-05,
      "loss": 0.8539,
      "step": 24300
    },
    {
      "epoch": 0.6006153846153847,
      "grad_norm": 5.001287460327148,
      "learning_rate": 2.6662685641871874e-05,
      "loss": 0.8493,
      "step": 24400
    },
    {
      "epoch": 0.6030769230769231,
      "grad_norm": 6.14833402633667,
      "learning_rate": 2.6635335089846565e-05,
      "loss": 0.8376,
      "step": 24500
    },
    {
      "epoch": 0.6055384615384616,
      "grad_norm": 6.781081676483154,
      "learning_rate": 2.660798453782126e-05,
      "loss": 0.8225,
      "step": 24600
    },
    {
      "epoch": 0.608,
      "grad_norm": 4.926576614379883,
      "learning_rate": 2.6580633985795946e-05,
      "loss": 0.8906,
      "step": 24700
    },
    {
      "epoch": 0.6104615384615385,
      "grad_norm": 4.4002299308776855,
      "learning_rate": 2.6553283433770636e-05,
      "loss": 0.8798,
      "step": 24800
    },
    {
      "epoch": 0.6129230769230769,
      "grad_norm": 9.61034107208252,
      "learning_rate": 2.652593288174533e-05,
      "loss": 0.8826,
      "step": 24900
    },
    {
      "epoch": 0.6153846153846154,
      "grad_norm": 5.974934101104736,
      "learning_rate": 2.649858232972002e-05,
      "loss": 0.8293,
      "step": 25000
    },
    {
      "epoch": 0.6178461538461538,
      "grad_norm": 7.145925521850586,
      "learning_rate": 2.6471231777694715e-05,
      "loss": 0.8812,
      "step": 25100
    },
    {
      "epoch": 0.6203076923076923,
      "grad_norm": 4.5387864112854,
      "learning_rate": 2.6444154731189656e-05,
      "loss": 0.862,
      "step": 25200
    },
    {
      "epoch": 0.6227692307692307,
      "grad_norm": 5.344590187072754,
      "learning_rate": 2.641680417916435e-05,
      "loss": 0.8804,
      "step": 25300
    },
    {
      "epoch": 0.6252307692307693,
      "grad_norm": 7.205427646636963,
      "learning_rate": 2.638945362713904e-05,
      "loss": 0.8481,
      "step": 25400
    },
    {
      "epoch": 0.6276923076923077,
      "grad_norm": 8.299105644226074,
      "learning_rate": 2.636237658063399e-05,
      "loss": 0.8448,
      "step": 25500
    },
    {
      "epoch": 0.6301538461538462,
      "grad_norm": 5.112163543701172,
      "learning_rate": 2.633502602860868e-05,
      "loss": 0.8521,
      "step": 25600
    },
    {
      "epoch": 0.6326153846153846,
      "grad_norm": 8.349105834960938,
      "learning_rate": 2.630767547658337e-05,
      "loss": 0.8485,
      "step": 25700
    },
    {
      "epoch": 0.6350769230769231,
      "grad_norm": 5.04911994934082,
      "learning_rate": 2.628032492455806e-05,
      "loss": 0.8713,
      "step": 25800
    },
    {
      "epoch": 0.6375384615384615,
      "grad_norm": 3.2394230365753174,
      "learning_rate": 2.625297437253275e-05,
      "loss": 0.8641,
      "step": 25900
    },
    {
      "epoch": 0.64,
      "grad_norm": 4.202975749969482,
      "learning_rate": 2.6225623820507444e-05,
      "loss": 0.8847,
      "step": 26000
    },
    {
      "epoch": 0.6424615384615384,
      "grad_norm": 6.276097774505615,
      "learning_rate": 2.6198273268482135e-05,
      "loss": 0.8413,
      "step": 26100
    },
    {
      "epoch": 0.6449230769230769,
      "grad_norm": 5.874493598937988,
      "learning_rate": 2.617092271645683e-05,
      "loss": 0.8279,
      "step": 26200
    },
    {
      "epoch": 0.6473846153846153,
      "grad_norm": 6.064974784851074,
      "learning_rate": 2.614357216443152e-05,
      "loss": 0.8379,
      "step": 26300
    },
    {
      "epoch": 0.6498461538461539,
      "grad_norm": 6.584208965301514,
      "learning_rate": 2.611622161240621e-05,
      "loss": 0.8715,
      "step": 26400
    },
    {
      "epoch": 0.6523076923076923,
      "grad_norm": 5.420483589172363,
      "learning_rate": 2.6088871060380904e-05,
      "loss": 0.8678,
      "step": 26500
    },
    {
      "epoch": 0.6547692307692308,
      "grad_norm": 6.2419352531433105,
      "learning_rate": 2.606179401387585e-05,
      "loss": 0.8392,
      "step": 26600
    },
    {
      "epoch": 0.6572307692307693,
      "grad_norm": 3.4390366077423096,
      "learning_rate": 2.603444346185054e-05,
      "loss": 0.8363,
      "step": 26700
    },
    {
      "epoch": 0.6596923076923077,
      "grad_norm": 6.717205047607422,
      "learning_rate": 2.600709290982523e-05,
      "loss": 0.8289,
      "step": 26800
    },
    {
      "epoch": 0.6621538461538462,
      "grad_norm": 8.266297340393066,
      "learning_rate": 2.5979742357799923e-05,
      "loss": 0.8384,
      "step": 26900
    },
    {
      "epoch": 0.6646153846153846,
      "grad_norm": 5.273908615112305,
      "learning_rate": 2.5952391805774614e-05,
      "loss": 0.8635,
      "step": 27000
    },
    {
      "epoch": 0.6670769230769231,
      "grad_norm": 4.438800811767578,
      "learning_rate": 2.5925041253749308e-05,
      "loss": 0.8581,
      "step": 27100
    },
    {
      "epoch": 0.6695384615384615,
      "grad_norm": 4.484886646270752,
      "learning_rate": 2.5897690701724e-05,
      "loss": 0.8503,
      "step": 27200
    },
    {
      "epoch": 0.672,
      "grad_norm": 4.047740459442139,
      "learning_rate": 2.587034014969869e-05,
      "loss": 0.8452,
      "step": 27300
    },
    {
      "epoch": 0.6744615384615384,
      "grad_norm": 6.717872619628906,
      "learning_rate": 2.584298959767338e-05,
      "loss": 0.8301,
      "step": 27400
    },
    {
      "epoch": 0.676923076923077,
      "grad_norm": 6.650742530822754,
      "learning_rate": 2.581563904564807e-05,
      "loss": 0.8335,
      "step": 27500
    },
    {
      "epoch": 0.6793846153846154,
      "grad_norm": 8.610350608825684,
      "learning_rate": 2.5788288493622764e-05,
      "loss": 0.8401,
      "step": 27600
    },
    {
      "epoch": 0.6818461538461539,
      "grad_norm": 6.40578031539917,
      "learning_rate": 2.5760937941597454e-05,
      "loss": 0.8621,
      "step": 27700
    },
    {
      "epoch": 0.6843076923076923,
      "grad_norm": 7.057775020599365,
      "learning_rate": 2.5733587389572145e-05,
      "loss": 0.8657,
      "step": 27800
    },
    {
      "epoch": 0.6867692307692308,
      "grad_norm": 8.423760414123535,
      "learning_rate": 2.570623683754684e-05,
      "loss": 0.865,
      "step": 27900
    },
    {
      "epoch": 0.6892307692307692,
      "grad_norm": 7.079285621643066,
      "learning_rate": 2.567888628552153e-05,
      "loss": 0.8806,
      "step": 28000
    },
    {
      "epoch": 0.6916923076923077,
      "grad_norm": 6.789994716644287,
      "learning_rate": 2.5651535733496223e-05,
      "loss": 0.8691,
      "step": 28100
    },
    {
      "epoch": 0.6941538461538461,
      "grad_norm": 7.473293304443359,
      "learning_rate": 2.5624185181470914e-05,
      "loss": 0.8405,
      "step": 28200
    },
    {
      "epoch": 0.6966153846153846,
      "grad_norm": 5.761188507080078,
      "learning_rate": 2.5596834629445604e-05,
      "loss": 0.872,
      "step": 28300
    },
    {
      "epoch": 0.699076923076923,
      "grad_norm": 6.680656909942627,
      "learning_rate": 2.5569484077420298e-05,
      "loss": 0.8404,
      "step": 28400
    },
    {
      "epoch": 0.7015384615384616,
      "grad_norm": 3.175565719604492,
      "learning_rate": 2.554213352539499e-05,
      "loss": 0.8564,
      "step": 28500
    },
    {
      "epoch": 0.704,
      "grad_norm": 3.8190536499023438,
      "learning_rate": 2.5514782973369682e-05,
      "loss": 0.842,
      "step": 28600
    },
    {
      "epoch": 0.7064615384615385,
      "grad_norm": 7.305816650390625,
      "learning_rate": 2.548743242134437e-05,
      "loss": 0.8276,
      "step": 28700
    },
    {
      "epoch": 0.7089230769230769,
      "grad_norm": 4.9040117263793945,
      "learning_rate": 2.546008186931906e-05,
      "loss": 0.8993,
      "step": 28800
    },
    {
      "epoch": 0.7113846153846154,
      "grad_norm": 4.685307025909424,
      "learning_rate": 2.5432731317293754e-05,
      "loss": 0.8561,
      "step": 28900
    },
    {
      "epoch": 0.7138461538461538,
      "grad_norm": 7.529599666595459,
      "learning_rate": 2.5405380765268445e-05,
      "loss": 0.8605,
      "step": 29000
    },
    {
      "epoch": 0.7163076923076923,
      "grad_norm": 5.133190631866455,
      "learning_rate": 2.537803021324314e-05,
      "loss": 0.8459,
      "step": 29100
    },
    {
      "epoch": 0.7187692307692307,
      "grad_norm": 5.182768821716309,
      "learning_rate": 2.535067966121783e-05,
      "loss": 0.8516,
      "step": 29200
    },
    {
      "epoch": 0.7212307692307692,
      "grad_norm": 10.573076248168945,
      "learning_rate": 2.5323329109192523e-05,
      "loss": 0.8468,
      "step": 29300
    },
    {
      "epoch": 0.7236923076923077,
      "grad_norm": 4.856940746307373,
      "learning_rate": 2.5295978557167213e-05,
      "loss": 0.8372,
      "step": 29400
    },
    {
      "epoch": 0.7261538461538461,
      "grad_norm": 3.597017526626587,
      "learning_rate": 2.5268628005141904e-05,
      "loss": 0.8505,
      "step": 29500
    },
    {
      "epoch": 0.7286153846153847,
      "grad_norm": 4.799380779266357,
      "learning_rate": 2.5241277453116598e-05,
      "loss": 0.8292,
      "step": 29600
    },
    {
      "epoch": 0.7310769230769231,
      "grad_norm": 7.952901363372803,
      "learning_rate": 2.521392690109129e-05,
      "loss": 0.853,
      "step": 29700
    },
    {
      "epoch": 0.7335384615384616,
      "grad_norm": 5.639937400817871,
      "learning_rate": 2.5186576349065982e-05,
      "loss": 0.8622,
      "step": 29800
    },
    {
      "epoch": 0.736,
      "grad_norm": 5.596823692321777,
      "learning_rate": 2.5159225797040673e-05,
      "loss": 0.8152,
      "step": 29900
    },
    {
      "epoch": 0.7384615384615385,
      "grad_norm": 6.694178104400635,
      "learning_rate": 2.513187524501536e-05,
      "loss": 0.8466,
      "step": 30000
    },
    {
      "epoch": 0.7409230769230769,
      "grad_norm": 6.857669830322266,
      "learning_rate": 2.5104524692990054e-05,
      "loss": 0.8625,
      "step": 30100
    },
    {
      "epoch": 0.7433846153846154,
      "grad_norm": 4.218230724334717,
      "learning_rate": 2.5077174140964744e-05,
      "loss": 0.8535,
      "step": 30200
    },
    {
      "epoch": 0.7458461538461538,
      "grad_norm": 6.734097003936768,
      "learning_rate": 2.5049823588939438e-05,
      "loss": 0.8357,
      "step": 30300
    },
    {
      "epoch": 0.7483076923076923,
      "grad_norm": 4.837639808654785,
      "learning_rate": 2.502247303691413e-05,
      "loss": 0.8633,
      "step": 30400
    },
    {
      "epoch": 0.7507692307692307,
      "grad_norm": 5.423983573913574,
      "learning_rate": 2.499512248488882e-05,
      "loss": 0.8618,
      "step": 30500
    },
    {
      "epoch": 0.7532307692307693,
      "grad_norm": 6.067203521728516,
      "learning_rate": 2.4967771932863513e-05,
      "loss": 0.8652,
      "step": 30600
    },
    {
      "epoch": 0.7556923076923077,
      "grad_norm": 7.195530891418457,
      "learning_rate": 2.4940421380838204e-05,
      "loss": 0.8493,
      "step": 30700
    },
    {
      "epoch": 0.7581538461538462,
      "grad_norm": 3.334568977355957,
      "learning_rate": 2.4913070828812898e-05,
      "loss": 0.819,
      "step": 30800
    },
    {
      "epoch": 0.7606153846153846,
      "grad_norm": 5.933940887451172,
      "learning_rate": 2.4885720276787588e-05,
      "loss": 0.8437,
      "step": 30900
    },
    {
      "epoch": 0.7630769230769231,
      "grad_norm": 5.752172946929932,
      "learning_rate": 2.485836972476228e-05,
      "loss": 0.8444,
      "step": 31000
    },
    {
      "epoch": 0.7655384615384615,
      "grad_norm": 5.028537273406982,
      "learning_rate": 2.4831292678257223e-05,
      "loss": 0.8691,
      "step": 31100
    },
    {
      "epoch": 0.768,
      "grad_norm": 6.22089958190918,
      "learning_rate": 2.4803942126231917e-05,
      "loss": 0.8437,
      "step": 31200
    },
    {
      "epoch": 0.7704615384615384,
      "grad_norm": 4.809950351715088,
      "learning_rate": 2.4776591574206608e-05,
      "loss": 0.8235,
      "step": 31300
    },
    {
      "epoch": 0.7729230769230769,
      "grad_norm": 4.0625739097595215,
      "learning_rate": 2.4749241022181298e-05,
      "loss": 0.8349,
      "step": 31400
    },
    {
      "epoch": 0.7753846153846153,
      "grad_norm": 6.184283256530762,
      "learning_rate": 2.4721890470155992e-05,
      "loss": 0.8352,
      "step": 31500
    },
    {
      "epoch": 0.7778461538461539,
      "grad_norm": 4.40026330947876,
      "learning_rate": 2.4694539918130683e-05,
      "loss": 0.8363,
      "step": 31600
    },
    {
      "epoch": 0.7803076923076923,
      "grad_norm": 5.038366794586182,
      "learning_rate": 2.4667189366105373e-05,
      "loss": 0.8265,
      "step": 31700
    },
    {
      "epoch": 0.7827692307692308,
      "grad_norm": 6.374381065368652,
      "learning_rate": 2.4639838814080064e-05,
      "loss": 0.8297,
      "step": 31800
    },
    {
      "epoch": 0.7852307692307692,
      "grad_norm": 5.148290634155273,
      "learning_rate": 2.4612488262054754e-05,
      "loss": 0.8497,
      "step": 31900
    },
    {
      "epoch": 0.7876923076923077,
      "grad_norm": 8.670260429382324,
      "learning_rate": 2.4585137710029448e-05,
      "loss": 0.8197,
      "step": 32000
    },
    {
      "epoch": 0.7901538461538462,
      "grad_norm": 6.785609722137451,
      "learning_rate": 2.455778715800414e-05,
      "loss": 0.8656,
      "step": 32100
    },
    {
      "epoch": 0.7926153846153846,
      "grad_norm": 8.464308738708496,
      "learning_rate": 2.4530436605978832e-05,
      "loss": 0.8244,
      "step": 32200
    },
    {
      "epoch": 0.7950769230769231,
      "grad_norm": 5.103969573974609,
      "learning_rate": 2.4503086053953523e-05,
      "loss": 0.8686,
      "step": 32300
    },
    {
      "epoch": 0.7975384615384615,
      "grad_norm": 7.254590034484863,
      "learning_rate": 2.4475735501928213e-05,
      "loss": 0.8418,
      "step": 32400
    },
    {
      "epoch": 0.8,
      "grad_norm": 4.278834342956543,
      "learning_rate": 2.4448384949902907e-05,
      "loss": 0.8259,
      "step": 32500
    },
    {
      "epoch": 0.8024615384615384,
      "grad_norm": 5.856122970581055,
      "learning_rate": 2.4421034397877598e-05,
      "loss": 0.8254,
      "step": 32600
    },
    {
      "epoch": 0.804923076923077,
      "grad_norm": 6.895452499389648,
      "learning_rate": 2.4393683845852292e-05,
      "loss": 0.8822,
      "step": 32700
    },
    {
      "epoch": 0.8073846153846154,
      "grad_norm": 5.856568813323975,
      "learning_rate": 2.4366333293826982e-05,
      "loss": 0.8481,
      "step": 32800
    },
    {
      "epoch": 0.8098461538461539,
      "grad_norm": 4.173523902893066,
      "learning_rate": 2.4338982741801673e-05,
      "loss": 0.8483,
      "step": 32900
    },
    {
      "epoch": 0.8123076923076923,
      "grad_norm": 9.99044418334961,
      "learning_rate": 2.4311632189776363e-05,
      "loss": 0.8104,
      "step": 33000
    },
    {
      "epoch": 0.8147692307692308,
      "grad_norm": 6.480630397796631,
      "learning_rate": 2.4284281637751054e-05,
      "loss": 0.8556,
      "step": 33100
    },
    {
      "epoch": 0.8172307692307692,
      "grad_norm": 4.317365646362305,
      "learning_rate": 2.4256931085725748e-05,
      "loss": 0.8453,
      "step": 33200
    },
    {
      "epoch": 0.8196923076923077,
      "grad_norm": 6.813265800476074,
      "learning_rate": 2.4229580533700438e-05,
      "loss": 0.817,
      "step": 33300
    },
    {
      "epoch": 0.8221538461538461,
      "grad_norm": 3.1132607460021973,
      "learning_rate": 2.420222998167513e-05,
      "loss": 0.8335,
      "step": 33400
    },
    {
      "epoch": 0.8246153846153846,
      "grad_norm": 9.204913139343262,
      "learning_rate": 2.4174879429649823e-05,
      "loss": 0.8323,
      "step": 33500
    },
    {
      "epoch": 0.827076923076923,
      "grad_norm": 3.6297504901885986,
      "learning_rate": 2.4147528877624513e-05,
      "loss": 0.8585,
      "step": 33600
    },
    {
      "epoch": 0.8295384615384616,
      "grad_norm": 4.20897912979126,
      "learning_rate": 2.4120178325599207e-05,
      "loss": 0.8641,
      "step": 33700
    },
    {
      "epoch": 0.832,
      "grad_norm": 8.481836318969727,
      "learning_rate": 2.4092827773573898e-05,
      "loss": 0.8329,
      "step": 33800
    },
    {
      "epoch": 0.8344615384615385,
      "grad_norm": 7.147228717803955,
      "learning_rate": 2.4065477221548588e-05,
      "loss": 0.8056,
      "step": 33900
    },
    {
      "epoch": 0.8369230769230769,
      "grad_norm": 4.363529682159424,
      "learning_rate": 2.4038126669523282e-05,
      "loss": 0.8222,
      "step": 34000
    },
    {
      "epoch": 0.8393846153846154,
      "grad_norm": 3.999385118484497,
      "learning_rate": 2.4010776117497973e-05,
      "loss": 0.8437,
      "step": 34100
    },
    {
      "epoch": 0.8418461538461538,
      "grad_norm": 5.745099067687988,
      "learning_rate": 2.3983425565472666e-05,
      "loss": 0.8569,
      "step": 34200
    },
    {
      "epoch": 0.8443076923076923,
      "grad_norm": 4.290913105010986,
      "learning_rate": 2.3956348518967608e-05,
      "loss": 0.8245,
      "step": 34300
    },
    {
      "epoch": 0.8467692307692307,
      "grad_norm": 4.728063583374023,
      "learning_rate": 2.39289979669423e-05,
      "loss": 0.8644,
      "step": 34400
    },
    {
      "epoch": 0.8492307692307692,
      "grad_norm": 7.446178913116455,
      "learning_rate": 2.3901647414916992e-05,
      "loss": 0.8229,
      "step": 34500
    },
    {
      "epoch": 0.8516923076923076,
      "grad_norm": 3.859654188156128,
      "learning_rate": 2.3874296862891686e-05,
      "loss": 0.8319,
      "step": 34600
    },
    {
      "epoch": 0.8541538461538462,
      "grad_norm": 7.01887845993042,
      "learning_rate": 2.3846946310866377e-05,
      "loss": 0.8249,
      "step": 34700
    },
    {
      "epoch": 0.8566153846153847,
      "grad_norm": 6.422252178192139,
      "learning_rate": 2.3819595758841064e-05,
      "loss": 0.8098,
      "step": 34800
    },
    {
      "epoch": 0.8590769230769231,
      "grad_norm": 5.261934280395508,
      "learning_rate": 2.3792245206815758e-05,
      "loss": 0.8335,
      "step": 34900
    },
    {
      "epoch": 0.8615384615384616,
      "grad_norm": 5.499969959259033,
      "learning_rate": 2.3764894654790448e-05,
      "loss": 0.8412,
      "step": 35000
    },
    {
      "epoch": 0.864,
      "grad_norm": 4.179421901702881,
      "learning_rate": 2.3737544102765142e-05,
      "loss": 0.8288,
      "step": 35100
    },
    {
      "epoch": 0.8664615384615385,
      "grad_norm": 5.440035343170166,
      "learning_rate": 2.3710193550739833e-05,
      "loss": 0.8144,
      "step": 35200
    },
    {
      "epoch": 0.8689230769230769,
      "grad_norm": 4.9527106285095215,
      "learning_rate": 2.3682842998714523e-05,
      "loss": 0.8239,
      "step": 35300
    },
    {
      "epoch": 0.8713846153846154,
      "grad_norm": 4.995861530303955,
      "learning_rate": 2.3655492446689217e-05,
      "loss": 0.8229,
      "step": 35400
    },
    {
      "epoch": 0.8738461538461538,
      "grad_norm": 5.349189281463623,
      "learning_rate": 2.3628141894663908e-05,
      "loss": 0.8744,
      "step": 35500
    },
    {
      "epoch": 0.8763076923076923,
      "grad_norm": 5.004942893981934,
      "learning_rate": 2.36007913426386e-05,
      "loss": 0.842,
      "step": 35600
    },
    {
      "epoch": 0.8787692307692307,
      "grad_norm": 8.425192832946777,
      "learning_rate": 2.3573440790613292e-05,
      "loss": 0.8452,
      "step": 35700
    },
    {
      "epoch": 0.8812307692307693,
      "grad_norm": 4.5245866775512695,
      "learning_rate": 2.3546090238587982e-05,
      "loss": 0.8414,
      "step": 35800
    },
    {
      "epoch": 0.8836923076923077,
      "grad_norm": 6.075411319732666,
      "learning_rate": 2.3518739686562676e-05,
      "loss": 0.8248,
      "step": 35900
    },
    {
      "epoch": 0.8861538461538462,
      "grad_norm": 5.852391719818115,
      "learning_rate": 2.3491389134537363e-05,
      "loss": 0.8558,
      "step": 36000
    },
    {
      "epoch": 0.8886153846153846,
      "grad_norm": 5.00699520111084,
      "learning_rate": 2.3464038582512057e-05,
      "loss": 0.8448,
      "step": 36100
    },
    {
      "epoch": 0.8910769230769231,
      "grad_norm": 4.858816623687744,
      "learning_rate": 2.3436688030486748e-05,
      "loss": 0.8385,
      "step": 36200
    },
    {
      "epoch": 0.8935384615384615,
      "grad_norm": 13.204228401184082,
      "learning_rate": 2.340933747846144e-05,
      "loss": 0.8152,
      "step": 36300
    },
    {
      "epoch": 0.896,
      "grad_norm": 6.549598217010498,
      "learning_rate": 2.3381986926436132e-05,
      "loss": 0.8443,
      "step": 36400
    },
    {
      "epoch": 0.8984615384615384,
      "grad_norm": 5.528799057006836,
      "learning_rate": 2.3354636374410823e-05,
      "loss": 0.8022,
      "step": 36500
    },
    {
      "epoch": 0.9009230769230769,
      "grad_norm": 4.864378452301025,
      "learning_rate": 2.3327285822385517e-05,
      "loss": 0.8241,
      "step": 36600
    },
    {
      "epoch": 0.9033846153846153,
      "grad_norm": 6.54362154006958,
      "learning_rate": 2.3299935270360207e-05,
      "loss": 0.8341,
      "step": 36700
    },
    {
      "epoch": 0.9058461538461539,
      "grad_norm": 4.8211188316345215,
      "learning_rate": 2.3272584718334898e-05,
      "loss": 0.8454,
      "step": 36800
    },
    {
      "epoch": 0.9083076923076923,
      "grad_norm": 5.003918647766113,
      "learning_rate": 2.324523416630959e-05,
      "loss": 0.8461,
      "step": 36900
    },
    {
      "epoch": 0.9107692307692308,
      "grad_norm": 4.792145729064941,
      "learning_rate": 2.3217883614284282e-05,
      "loss": 0.8362,
      "step": 37000
    },
    {
      "epoch": 0.9132307692307692,
      "grad_norm": 6.566333770751953,
      "learning_rate": 2.3190533062258976e-05,
      "loss": 0.858,
      "step": 37100
    },
    {
      "epoch": 0.9156923076923077,
      "grad_norm": 4.183631420135498,
      "learning_rate": 2.3163182510233667e-05,
      "loss": 0.8419,
      "step": 37200
    },
    {
      "epoch": 0.9181538461538462,
      "grad_norm": 4.597609519958496,
      "learning_rate": 2.3135831958208357e-05,
      "loss": 0.8055,
      "step": 37300
    },
    {
      "epoch": 0.9206153846153846,
      "grad_norm": 4.6737751960754395,
      "learning_rate": 2.3108481406183048e-05,
      "loss": 0.8053,
      "step": 37400
    },
    {
      "epoch": 0.9230769230769231,
      "grad_norm": 6.0085225105285645,
      "learning_rate": 2.3081130854157738e-05,
      "loss": 0.8095,
      "step": 37500
    },
    {
      "epoch": 0.9255384615384615,
      "grad_norm": 8.44591999053955,
      "learning_rate": 2.3053780302132432e-05,
      "loss": 0.8027,
      "step": 37600
    },
    {
      "epoch": 0.928,
      "grad_norm": 4.6963396072387695,
      "learning_rate": 2.3026429750107123e-05,
      "loss": 0.8263,
      "step": 37700
    },
    {
      "epoch": 0.9304615384615385,
      "grad_norm": 6.188713073730469,
      "learning_rate": 2.2999079198081816e-05,
      "loss": 0.8268,
      "step": 37800
    },
    {
      "epoch": 0.932923076923077,
      "grad_norm": 5.544410228729248,
      "learning_rate": 2.2971728646056507e-05,
      "loss": 0.8286,
      "step": 37900
    },
    {
      "epoch": 0.9353846153846154,
      "grad_norm": 4.945823669433594,
      "learning_rate": 2.2944378094031198e-05,
      "loss": 0.8163,
      "step": 38000
    },
    {
      "epoch": 0.9378461538461539,
      "grad_norm": 5.224424839019775,
      "learning_rate": 2.291702754200589e-05,
      "loss": 0.8432,
      "step": 38100
    },
    {
      "epoch": 0.9403076923076923,
      "grad_norm": 4.1173095703125,
      "learning_rate": 2.2889676989980582e-05,
      "loss": 0.832,
      "step": 38200
    },
    {
      "epoch": 0.9427692307692308,
      "grad_norm": 6.54420280456543,
      "learning_rate": 2.2862599943475527e-05,
      "loss": 0.828,
      "step": 38300
    },
    {
      "epoch": 0.9452307692307692,
      "grad_norm": 6.487151622772217,
      "learning_rate": 2.2835249391450217e-05,
      "loss": 0.8422,
      "step": 38400
    },
    {
      "epoch": 0.9476923076923077,
      "grad_norm": 4.637773513793945,
      "learning_rate": 2.280789883942491e-05,
      "loss": 0.8285,
      "step": 38500
    },
    {
      "epoch": 0.9501538461538461,
      "grad_norm": 3.6679646968841553,
      "learning_rate": 2.27805482873996e-05,
      "loss": 0.8381,
      "step": 38600
    },
    {
      "epoch": 0.9526153846153846,
      "grad_norm": 5.499706268310547,
      "learning_rate": 2.2753197735374292e-05,
      "loss": 0.8296,
      "step": 38700
    },
    {
      "epoch": 0.955076923076923,
      "grad_norm": 3.5953705310821533,
      "learning_rate": 2.2725847183348986e-05,
      "loss": 0.8684,
      "step": 38800
    },
    {
      "epoch": 0.9575384615384616,
      "grad_norm": 4.05959415435791,
      "learning_rate": 2.269877013684393e-05,
      "loss": 0.8282,
      "step": 38900
    },
    {
      "epoch": 0.96,
      "grad_norm": 6.606021404266357,
      "learning_rate": 2.267141958481862e-05,
      "loss": 0.8133,
      "step": 39000
    },
    {
      "epoch": 0.9624615384615385,
      "grad_norm": 8.810685157775879,
      "learning_rate": 2.264406903279331e-05,
      "loss": 0.8523,
      "step": 39100
    },
    {
      "epoch": 0.9649230769230769,
      "grad_norm": 6.22053861618042,
      "learning_rate": 2.2616718480768006e-05,
      "loss": 0.8322,
      "step": 39200
    },
    {
      "epoch": 0.9673846153846154,
      "grad_norm": 4.5009541511535645,
      "learning_rate": 2.2589367928742696e-05,
      "loss": 0.8342,
      "step": 39300
    },
    {
      "epoch": 0.9698461538461538,
      "grad_norm": 4.694104194641113,
      "learning_rate": 2.256201737671739e-05,
      "loss": 0.8383,
      "step": 39400
    },
    {
      "epoch": 0.9723076923076923,
      "grad_norm": 7.546502590179443,
      "learning_rate": 2.2534666824692077e-05,
      "loss": 0.7901,
      "step": 39500
    },
    {
      "epoch": 0.9747692307692307,
      "grad_norm": 5.669527053833008,
      "learning_rate": 2.2507316272666768e-05,
      "loss": 0.8156,
      "step": 39600
    },
    {
      "epoch": 0.9772307692307692,
      "grad_norm": 4.497435569763184,
      "learning_rate": 2.247996572064146e-05,
      "loss": 0.8491,
      "step": 39700
    },
    {
      "epoch": 0.9796923076923076,
      "grad_norm": 7.621734619140625,
      "learning_rate": 2.2452615168616152e-05,
      "loss": 0.8246,
      "step": 39800
    },
    {
      "epoch": 0.9821538461538462,
      "grad_norm": 6.436806678771973,
      "learning_rate": 2.2425264616590846e-05,
      "loss": 0.8572,
      "step": 39900
    },
    {
      "epoch": 0.9846153846153847,
      "grad_norm": 6.638245105743408,
      "learning_rate": 2.2397914064565536e-05,
      "loss": 0.8058,
      "step": 40000
    },
    {
      "epoch": 0.9870769230769231,
      "grad_norm": 5.76431941986084,
      "learning_rate": 2.2370563512540227e-05,
      "loss": 0.7916,
      "step": 40100
    },
    {
      "epoch": 0.9895384615384616,
      "grad_norm": 7.0058979988098145,
      "learning_rate": 2.234321296051492e-05,
      "loss": 0.8008,
      "step": 40200
    },
    {
      "epoch": 0.992,
      "grad_norm": 6.531593322753906,
      "learning_rate": 2.231586240848961e-05,
      "loss": 0.8329,
      "step": 40300
    },
    {
      "epoch": 0.9944615384615385,
      "grad_norm": 10.018996238708496,
      "learning_rate": 2.2288511856464305e-05,
      "loss": 0.822,
      "step": 40400
    },
    {
      "epoch": 0.9969230769230769,
      "grad_norm": 5.069989204406738,
      "learning_rate": 2.2261161304438996e-05,
      "loss": 0.8561,
      "step": 40500
    },
    {
      "epoch": 0.9993846153846154,
      "grad_norm": 4.467319965362549,
      "learning_rate": 2.2233810752413686e-05,
      "loss": 0.8076,
      "step": 40600
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.64476,
      "eval_f1": 0.6453001389362848,
      "eval_loss": 0.8222187757492065,
      "eval_precision": 0.6461790448384389,
      "eval_recall": 0.64476,
      "eval_runtime": 10.2737,
      "eval_samples_per_second": 4866.801,
      "eval_steps_per_second": 304.175,
      "step": 40625
    },
    {
      "epoch": 1.0018461538461538,
      "grad_norm": 4.761404514312744,
      "learning_rate": 2.220646020038838e-05,
      "loss": 0.786,
      "step": 40700
    },
    {
      "epoch": 1.0043076923076923,
      "grad_norm": 5.60245418548584,
      "learning_rate": 2.2179109648363067e-05,
      "loss": 0.7826,
      "step": 40800
    },
    {
      "epoch": 1.0067692307692309,
      "grad_norm": 3.983217477798462,
      "learning_rate": 2.215175909633776e-05,
      "loss": 0.7745,
      "step": 40900
    },
    {
      "epoch": 1.0092307692307692,
      "grad_norm": 5.327173233032227,
      "learning_rate": 2.2124408544312452e-05,
      "loss": 0.7774,
      "step": 41000
    },
    {
      "epoch": 1.0116923076923077,
      "grad_norm": 6.527793884277344,
      "learning_rate": 2.2097057992287146e-05,
      "loss": 0.7429,
      "step": 41100
    },
    {
      "epoch": 1.0141538461538462,
      "grad_norm": 4.883205413818359,
      "learning_rate": 2.2069707440261836e-05,
      "loss": 0.7622,
      "step": 41200
    },
    {
      "epoch": 1.0166153846153847,
      "grad_norm": 9.62704086303711,
      "learning_rate": 2.2042356888236527e-05,
      "loss": 0.792,
      "step": 41300
    },
    {
      "epoch": 1.019076923076923,
      "grad_norm": 3.2841672897338867,
      "learning_rate": 2.201500633621122e-05,
      "loss": 0.7912,
      "step": 41400
    },
    {
      "epoch": 1.0215384615384615,
      "grad_norm": 3.276426315307617,
      "learning_rate": 2.198765578418591e-05,
      "loss": 0.7876,
      "step": 41500
    },
    {
      "epoch": 1.024,
      "grad_norm": 5.347899436950684,
      "learning_rate": 2.1960305232160605e-05,
      "loss": 0.7816,
      "step": 41600
    },
    {
      "epoch": 1.0264615384615385,
      "grad_norm": 4.840723037719727,
      "learning_rate": 2.1932954680135296e-05,
      "loss": 0.7548,
      "step": 41700
    },
    {
      "epoch": 1.0289230769230768,
      "grad_norm": 8.042017936706543,
      "learning_rate": 2.1905604128109986e-05,
      "loss": 0.7655,
      "step": 41800
    },
    {
      "epoch": 1.0313846153846153,
      "grad_norm": 7.528895378112793,
      "learning_rate": 2.187825357608468e-05,
      "loss": 0.7662,
      "step": 41900
    },
    {
      "epoch": 1.0338461538461539,
      "grad_norm": 9.079379081726074,
      "learning_rate": 2.185090302405937e-05,
      "loss": 0.787,
      "step": 42000
    },
    {
      "epoch": 1.0363076923076924,
      "grad_norm": 8.270055770874023,
      "learning_rate": 2.182355247203406e-05,
      "loss": 0.7848,
      "step": 42100
    },
    {
      "epoch": 1.0387692307692307,
      "grad_norm": 7.5418291091918945,
      "learning_rate": 2.179620192000875e-05,
      "loss": 0.7428,
      "step": 42200
    },
    {
      "epoch": 1.0412307692307692,
      "grad_norm": 9.259247779846191,
      "learning_rate": 2.1768851367983442e-05,
      "loss": 0.7598,
      "step": 42300
    },
    {
      "epoch": 1.0436923076923077,
      "grad_norm": 6.641939163208008,
      "learning_rate": 2.1741500815958136e-05,
      "loss": 0.7843,
      "step": 42400
    },
    {
      "epoch": 1.0461538461538462,
      "grad_norm": 7.503288745880127,
      "learning_rate": 2.1714150263932826e-05,
      "loss": 0.765,
      "step": 42500
    },
    {
      "epoch": 1.0486153846153847,
      "grad_norm": 4.242551803588867,
      "learning_rate": 2.168679971190752e-05,
      "loss": 0.7429,
      "step": 42600
    },
    {
      "epoch": 1.051076923076923,
      "grad_norm": 6.312413692474365,
      "learning_rate": 2.165944915988221e-05,
      "loss": 0.7483,
      "step": 42700
    },
    {
      "epoch": 1.0535384615384615,
      "grad_norm": 5.69278621673584,
      "learning_rate": 2.16320986078569e-05,
      "loss": 0.7337,
      "step": 42800
    },
    {
      "epoch": 1.056,
      "grad_norm": 6.967745780944824,
      "learning_rate": 2.1604748055831595e-05,
      "loss": 0.7655,
      "step": 42900
    },
    {
      "epoch": 1.0584615384615386,
      "grad_norm": 3.933806896209717,
      "learning_rate": 2.1577397503806286e-05,
      "loss": 0.7942,
      "step": 43000
    },
    {
      "epoch": 1.0609230769230769,
      "grad_norm": 5.6945319175720215,
      "learning_rate": 2.155004695178098e-05,
      "loss": 0.7838,
      "step": 43100
    },
    {
      "epoch": 1.0633846153846154,
      "grad_norm": 8.923783302307129,
      "learning_rate": 2.152269639975567e-05,
      "loss": 0.7817,
      "step": 43200
    },
    {
      "epoch": 1.0658461538461539,
      "grad_norm": 8.074753761291504,
      "learning_rate": 2.149534584773036e-05,
      "loss": 0.7423,
      "step": 43300
    },
    {
      "epoch": 1.0683076923076924,
      "grad_norm": 6.679840564727783,
      "learning_rate": 2.146799529570505e-05,
      "loss": 0.7495,
      "step": 43400
    },
    {
      "epoch": 1.0707692307692307,
      "grad_norm": 5.823739528656006,
      "learning_rate": 2.14409182492e-05,
      "loss": 0.7782,
      "step": 43500
    },
    {
      "epoch": 1.0732307692307692,
      "grad_norm": 7.633833885192871,
      "learning_rate": 2.141356769717469e-05,
      "loss": 0.7644,
      "step": 43600
    },
    {
      "epoch": 1.0756923076923077,
      "grad_norm": 7.088442325592041,
      "learning_rate": 2.138621714514938e-05,
      "loss": 0.7563,
      "step": 43700
    },
    {
      "epoch": 1.0781538461538462,
      "grad_norm": 7.488724231719971,
      "learning_rate": 2.1358866593124074e-05,
      "loss": 0.7441,
      "step": 43800
    },
    {
      "epoch": 1.0806153846153845,
      "grad_norm": 7.744662284851074,
      "learning_rate": 2.133151604109876e-05,
      "loss": 0.7933,
      "step": 43900
    },
    {
      "epoch": 1.083076923076923,
      "grad_norm": 8.85278606414795,
      "learning_rate": 2.1304165489073455e-05,
      "loss": 0.7574,
      "step": 44000
    },
    {
      "epoch": 1.0855384615384616,
      "grad_norm": 5.917246341705322,
      "learning_rate": 2.1276814937048146e-05,
      "loss": 0.7637,
      "step": 44100
    },
    {
      "epoch": 1.088,
      "grad_norm": 6.385264873504639,
      "learning_rate": 2.1249464385022836e-05,
      "loss": 0.7802,
      "step": 44200
    },
    {
      "epoch": 1.0904615384615384,
      "grad_norm": 6.521720886230469,
      "learning_rate": 2.122211383299753e-05,
      "loss": 0.7291,
      "step": 44300
    },
    {
      "epoch": 1.0929230769230769,
      "grad_norm": 6.33170223236084,
      "learning_rate": 2.119476328097222e-05,
      "loss": 0.7894,
      "step": 44400
    },
    {
      "epoch": 1.0953846153846154,
      "grad_norm": 4.813448429107666,
      "learning_rate": 2.1167412728946915e-05,
      "loss": 0.7626,
      "step": 44500
    },
    {
      "epoch": 1.097846153846154,
      "grad_norm": 6.68535852432251,
      "learning_rate": 2.1140062176921605e-05,
      "loss": 0.7709,
      "step": 44600
    },
    {
      "epoch": 1.1003076923076922,
      "grad_norm": 4.98701286315918,
      "learning_rate": 2.1112711624896296e-05,
      "loss": 0.7724,
      "step": 44700
    },
    {
      "epoch": 1.1027692307692307,
      "grad_norm": 6.625462532043457,
      "learning_rate": 2.108536107287099e-05,
      "loss": 0.7568,
      "step": 44800
    },
    {
      "epoch": 1.1052307692307692,
      "grad_norm": 5.637022495269775,
      "learning_rate": 2.105801052084568e-05,
      "loss": 0.7362,
      "step": 44900
    },
    {
      "epoch": 1.1076923076923078,
      "grad_norm": 4.536752700805664,
      "learning_rate": 2.1030659968820374e-05,
      "loss": 0.7958,
      "step": 45000
    },
    {
      "epoch": 1.110153846153846,
      "grad_norm": 4.991253852844238,
      "learning_rate": 2.100330941679506e-05,
      "loss": 0.7788,
      "step": 45100
    },
    {
      "epoch": 1.1126153846153846,
      "grad_norm": 7.345937728881836,
      "learning_rate": 2.097595886476975e-05,
      "loss": 0.7626,
      "step": 45200
    },
    {
      "epoch": 1.115076923076923,
      "grad_norm": 3.513054370880127,
      "learning_rate": 2.0948608312744446e-05,
      "loss": 0.7928,
      "step": 45300
    },
    {
      "epoch": 1.1175384615384616,
      "grad_norm": 6.191186428070068,
      "learning_rate": 2.0921257760719136e-05,
      "loss": 0.7981,
      "step": 45400
    },
    {
      "epoch": 1.12,
      "grad_norm": 9.051921844482422,
      "learning_rate": 2.089390720869383e-05,
      "loss": 0.7791,
      "step": 45500
    },
    {
      "epoch": 1.1224615384615384,
      "grad_norm": 5.61838436126709,
      "learning_rate": 2.086683016218877e-05,
      "loss": 0.7592,
      "step": 45600
    },
    {
      "epoch": 1.124923076923077,
      "grad_norm": 6.660464763641357,
      "learning_rate": 2.0839479610163465e-05,
      "loss": 0.7483,
      "step": 45700
    },
    {
      "epoch": 1.1273846153846154,
      "grad_norm": 4.276314735412598,
      "learning_rate": 2.0812129058138156e-05,
      "loss": 0.7912,
      "step": 45800
    },
    {
      "epoch": 1.1298461538461537,
      "grad_norm": 6.755815505981445,
      "learning_rate": 2.078477850611285e-05,
      "loss": 0.7593,
      "step": 45900
    },
    {
      "epoch": 1.1323076923076922,
      "grad_norm": 6.6401472091674805,
      "learning_rate": 2.075742795408754e-05,
      "loss": 0.7242,
      "step": 46000
    },
    {
      "epoch": 1.1347692307692308,
      "grad_norm": 5.5929741859436035,
      "learning_rate": 2.0730350907582485e-05,
      "loss": 0.762,
      "step": 46100
    },
    {
      "epoch": 1.1372307692307693,
      "grad_norm": 8.821449279785156,
      "learning_rate": 2.0703000355557175e-05,
      "loss": 0.77,
      "step": 46200
    },
    {
      "epoch": 1.1396923076923078,
      "grad_norm": 7.4466400146484375,
      "learning_rate": 2.067564980353187e-05,
      "loss": 0.7657,
      "step": 46300
    },
    {
      "epoch": 1.142153846153846,
      "grad_norm": 6.149731159210205,
      "learning_rate": 2.064829925150656e-05,
      "loss": 0.7875,
      "step": 46400
    },
    {
      "epoch": 1.1446153846153846,
      "grad_norm": 4.236447334289551,
      "learning_rate": 2.062094869948125e-05,
      "loss": 0.7401,
      "step": 46500
    },
    {
      "epoch": 1.147076923076923,
      "grad_norm": 6.470073223114014,
      "learning_rate": 2.0593598147455944e-05,
      "loss": 0.7737,
      "step": 46600
    },
    {
      "epoch": 1.1495384615384616,
      "grad_norm": 6.041823387145996,
      "learning_rate": 2.0566247595430635e-05,
      "loss": 0.7542,
      "step": 46700
    },
    {
      "epoch": 1.152,
      "grad_norm": 4.336523532867432,
      "learning_rate": 2.053889704340533e-05,
      "loss": 0.7592,
      "step": 46800
    },
    {
      "epoch": 1.1544615384615384,
      "grad_norm": 6.862964630126953,
      "learning_rate": 2.051154649138002e-05,
      "loss": 0.803,
      "step": 46900
    },
    {
      "epoch": 1.156923076923077,
      "grad_norm": 6.185791492462158,
      "learning_rate": 2.048419593935471e-05,
      "loss": 0.7602,
      "step": 47000
    },
    {
      "epoch": 1.1593846153846155,
      "grad_norm": 6.522503852844238,
      "learning_rate": 2.0456845387329403e-05,
      "loss": 0.7634,
      "step": 47100
    },
    {
      "epoch": 1.1618461538461538,
      "grad_norm": 8.096880912780762,
      "learning_rate": 2.0429494835304094e-05,
      "loss": 0.7722,
      "step": 47200
    },
    {
      "epoch": 1.1643076923076923,
      "grad_norm": 4.832889080047607,
      "learning_rate": 2.0402144283278788e-05,
      "loss": 0.7617,
      "step": 47300
    },
    {
      "epoch": 1.1667692307692308,
      "grad_norm": 4.841307640075684,
      "learning_rate": 2.0374793731253475e-05,
      "loss": 0.7625,
      "step": 47400
    },
    {
      "epoch": 1.1692307692307693,
      "grad_norm": 6.023287773132324,
      "learning_rate": 2.0347443179228166e-05,
      "loss": 0.7855,
      "step": 47500
    },
    {
      "epoch": 1.1716923076923078,
      "grad_norm": 5.065258979797363,
      "learning_rate": 2.032009262720286e-05,
      "loss": 0.7741,
      "step": 47600
    },
    {
      "epoch": 1.174153846153846,
      "grad_norm": 8.61488151550293,
      "learning_rate": 2.029274207517755e-05,
      "loss": 0.7465,
      "step": 47700
    },
    {
      "epoch": 1.1766153846153846,
      "grad_norm": 5.82024621963501,
      "learning_rate": 2.0265391523152244e-05,
      "loss": 0.7534,
      "step": 47800
    },
    {
      "epoch": 1.1790769230769231,
      "grad_norm": 6.611560821533203,
      "learning_rate": 2.0238040971126934e-05,
      "loss": 0.7578,
      "step": 47900
    },
    {
      "epoch": 1.1815384615384614,
      "grad_norm": 6.278874397277832,
      "learning_rate": 2.0210690419101625e-05,
      "loss": 0.7899,
      "step": 48000
    },
    {
      "epoch": 1.184,
      "grad_norm": 4.992583751678467,
      "learning_rate": 2.018333986707632e-05,
      "loss": 0.7402,
      "step": 48100
    },
    {
      "epoch": 1.1864615384615385,
      "grad_norm": 5.108243465423584,
      "learning_rate": 2.015598931505101e-05,
      "loss": 0.7502,
      "step": 48200
    },
    {
      "epoch": 1.188923076923077,
      "grad_norm": 4.358340263366699,
      "learning_rate": 2.0128638763025703e-05,
      "loss": 0.7464,
      "step": 48300
    },
    {
      "epoch": 1.1913846153846155,
      "grad_norm": 7.024172782897949,
      "learning_rate": 2.0101288211000394e-05,
      "loss": 0.7669,
      "step": 48400
    },
    {
      "epoch": 1.1938461538461538,
      "grad_norm": 6.5949602127075195,
      "learning_rate": 2.0073937658975084e-05,
      "loss": 0.8147,
      "step": 48500
    },
    {
      "epoch": 1.1963076923076923,
      "grad_norm": 6.39801025390625,
      "learning_rate": 2.0046587106949775e-05,
      "loss": 0.7584,
      "step": 48600
    },
    {
      "epoch": 1.1987692307692308,
      "grad_norm": 9.96389389038086,
      "learning_rate": 2.0019236554924465e-05,
      "loss": 0.7619,
      "step": 48700
    },
    {
      "epoch": 1.2012307692307693,
      "grad_norm": 4.999556064605713,
      "learning_rate": 1.999188600289916e-05,
      "loss": 0.7475,
      "step": 48800
    },
    {
      "epoch": 1.2036923076923076,
      "grad_norm": 8.068090438842773,
      "learning_rate": 1.996453545087385e-05,
      "loss": 0.7515,
      "step": 48900
    },
    {
      "epoch": 1.2061538461538461,
      "grad_norm": 7.314836025238037,
      "learning_rate": 1.993718489884854e-05,
      "loss": 0.7645,
      "step": 49000
    },
    {
      "epoch": 1.2086153846153846,
      "grad_norm": 8.011629104614258,
      "learning_rate": 1.9909834346823234e-05,
      "loss": 0.7709,
      "step": 49100
    },
    {
      "epoch": 1.2110769230769232,
      "grad_norm": 4.851783752441406,
      "learning_rate": 1.9882483794797925e-05,
      "loss": 0.7549,
      "step": 49200
    },
    {
      "epoch": 1.2135384615384615,
      "grad_norm": 4.589274883270264,
      "learning_rate": 1.985513324277262e-05,
      "loss": 0.793,
      "step": 49300
    },
    {
      "epoch": 1.216,
      "grad_norm": 5.0757975578308105,
      "learning_rate": 1.982805619626756e-05,
      "loss": 0.7824,
      "step": 49400
    },
    {
      "epoch": 1.2184615384615385,
      "grad_norm": 7.734333038330078,
      "learning_rate": 1.9800705644242254e-05,
      "loss": 0.7775,
      "step": 49500
    },
    {
      "epoch": 1.220923076923077,
      "grad_norm": 5.033624649047852,
      "learning_rate": 1.9773355092216944e-05,
      "loss": 0.7594,
      "step": 49600
    },
    {
      "epoch": 1.2233846153846153,
      "grad_norm": 4.916983604431152,
      "learning_rate": 1.9746004540191638e-05,
      "loss": 0.7185,
      "step": 49700
    },
    {
      "epoch": 1.2258461538461538,
      "grad_norm": 5.202444553375244,
      "learning_rate": 1.971865398816633e-05,
      "loss": 0.7624,
      "step": 49800
    },
    {
      "epoch": 1.2283076923076923,
      "grad_norm": 8.78739070892334,
      "learning_rate": 1.969130343614102e-05,
      "loss": 0.7778,
      "step": 49900
    },
    {
      "epoch": 1.2307692307692308,
      "grad_norm": 10.029391288757324,
      "learning_rate": 1.9663952884115713e-05,
      "loss": 0.7378,
      "step": 50000
    },
    {
      "epoch": 1.2332307692307691,
      "grad_norm": 7.036639213562012,
      "learning_rate": 1.9636602332090404e-05,
      "loss": 0.8017,
      "step": 50100
    },
    {
      "epoch": 1.2356923076923076,
      "grad_norm": 6.0666375160217285,
      "learning_rate": 1.9609251780065097e-05,
      "loss": 0.7967,
      "step": 50200
    },
    {
      "epoch": 1.2381538461538462,
      "grad_norm": 6.360649585723877,
      "learning_rate": 1.9581901228039788e-05,
      "loss": 0.7788,
      "step": 50300
    },
    {
      "epoch": 1.2406153846153847,
      "grad_norm": 7.586392402648926,
      "learning_rate": 1.9554550676014475e-05,
      "loss": 0.7819,
      "step": 50400
    },
    {
      "epoch": 1.2430769230769232,
      "grad_norm": 7.908998012542725,
      "learning_rate": 1.952720012398917e-05,
      "loss": 0.7429,
      "step": 50500
    },
    {
      "epoch": 1.2455384615384615,
      "grad_norm": 13.68016529083252,
      "learning_rate": 1.949984957196386e-05,
      "loss": 0.7673,
      "step": 50600
    },
    {
      "epoch": 1.248,
      "grad_norm": 12.900351524353027,
      "learning_rate": 1.9472499019938553e-05,
      "loss": 0.7516,
      "step": 50700
    },
    {
      "epoch": 1.2504615384615385,
      "grad_norm": 7.182750225067139,
      "learning_rate": 1.9445148467913244e-05,
      "loss": 0.7978,
      "step": 50800
    },
    {
      "epoch": 1.2529230769230768,
      "grad_norm": 6.644683837890625,
      "learning_rate": 1.9417797915887934e-05,
      "loss": 0.7963,
      "step": 50900
    },
    {
      "epoch": 1.2553846153846153,
      "grad_norm": 9.228448867797852,
      "learning_rate": 1.939044736386263e-05,
      "loss": 0.7747,
      "step": 51000
    },
    {
      "epoch": 1.2578461538461538,
      "grad_norm": 4.143022537231445,
      "learning_rate": 1.936309681183732e-05,
      "loss": 0.7463,
      "step": 51100
    },
    {
      "epoch": 1.2603076923076924,
      "grad_norm": 9.292423248291016,
      "learning_rate": 1.9335746259812013e-05,
      "loss": 0.7491,
      "step": 51200
    },
    {
      "epoch": 1.2627692307692309,
      "grad_norm": 7.6759233474731445,
      "learning_rate": 1.9308395707786703e-05,
      "loss": 0.7661,
      "step": 51300
    },
    {
      "epoch": 1.2652307692307692,
      "grad_norm": 4.324414253234863,
      "learning_rate": 1.9281045155761394e-05,
      "loss": 0.8256,
      "step": 51400
    },
    {
      "epoch": 1.2676923076923077,
      "grad_norm": 5.379696369171143,
      "learning_rate": 1.9253694603736088e-05,
      "loss": 0.7833,
      "step": 51500
    },
    {
      "epoch": 1.2701538461538462,
      "grad_norm": 8.976016998291016,
      "learning_rate": 1.9226344051710778e-05,
      "loss": 0.7777,
      "step": 51600
    },
    {
      "epoch": 1.2726153846153845,
      "grad_norm": 5.267280101776123,
      "learning_rate": 1.919899349968547e-05,
      "loss": 0.7918,
      "step": 51700
    },
    {
      "epoch": 1.275076923076923,
      "grad_norm": 4.651779651641846,
      "learning_rate": 1.917164294766016e-05,
      "loss": 0.8036,
      "step": 51800
    },
    {
      "epoch": 1.2775384615384615,
      "grad_norm": 5.422675132751465,
      "learning_rate": 1.914429239563485e-05,
      "loss": 0.7787,
      "step": 51900
    },
    {
      "epoch": 1.28,
      "grad_norm": 10.426899909973145,
      "learning_rate": 1.9116941843609544e-05,
      "loss": 0.7489,
      "step": 52000
    },
    {
      "epoch": 1.2824615384615385,
      "grad_norm": 4.220561981201172,
      "learning_rate": 1.9089591291584234e-05,
      "loss": 0.7632,
      "step": 52100
    },
    {
      "epoch": 1.2849230769230768,
      "grad_norm": 4.457458972930908,
      "learning_rate": 1.9062240739558928e-05,
      "loss": 0.7617,
      "step": 52200
    },
    {
      "epoch": 1.2873846153846153,
      "grad_norm": 6.72433614730835,
      "learning_rate": 1.903489018753362e-05,
      "loss": 0.7741,
      "step": 52300
    },
    {
      "epoch": 1.2898461538461539,
      "grad_norm": 3.591348648071289,
      "learning_rate": 1.9007813141028563e-05,
      "loss": 0.7691,
      "step": 52400
    },
    {
      "epoch": 1.2923076923076924,
      "grad_norm": 6.214156150817871,
      "learning_rate": 1.8980462589003254e-05,
      "loss": 0.7701,
      "step": 52500
    },
    {
      "epoch": 1.294769230769231,
      "grad_norm": 5.704122066497803,
      "learning_rate": 1.8953112036977948e-05,
      "loss": 0.7824,
      "step": 52600
    },
    {
      "epoch": 1.2972307692307692,
      "grad_norm": 10.775883674621582,
      "learning_rate": 1.8925761484952638e-05,
      "loss": 0.7849,
      "step": 52700
    },
    {
      "epoch": 1.2996923076923077,
      "grad_norm": 4.578551769256592,
      "learning_rate": 1.889841093292733e-05,
      "loss": 0.7539,
      "step": 52800
    },
    {
      "epoch": 1.3021538461538462,
      "grad_norm": 6.382841110229492,
      "learning_rate": 1.8871060380902023e-05,
      "loss": 0.7793,
      "step": 52900
    },
    {
      "epoch": 1.3046153846153845,
      "grad_norm": 12.143831253051758,
      "learning_rate": 1.8843709828876713e-05,
      "loss": 0.7728,
      "step": 53000
    },
    {
      "epoch": 1.307076923076923,
      "grad_norm": 6.769130229949951,
      "learning_rate": 1.8816359276851407e-05,
      "loss": 0.7436,
      "step": 53100
    },
    {
      "epoch": 1.3095384615384615,
      "grad_norm": 5.622079849243164,
      "learning_rate": 1.8789008724826098e-05,
      "loss": 0.7733,
      "step": 53200
    },
    {
      "epoch": 1.312,
      "grad_norm": 2.8283419609069824,
      "learning_rate": 1.8761658172800788e-05,
      "loss": 0.7454,
      "step": 53300
    },
    {
      "epoch": 1.3144615384615386,
      "grad_norm": 7.146626949310303,
      "learning_rate": 1.873430762077548e-05,
      "loss": 0.7421,
      "step": 53400
    },
    {
      "epoch": 1.3169230769230769,
      "grad_norm": 7.222152233123779,
      "learning_rate": 1.870695706875017e-05,
      "loss": 0.7781,
      "step": 53500
    },
    {
      "epoch": 1.3193846153846154,
      "grad_norm": 6.798827171325684,
      "learning_rate": 1.8679606516724863e-05,
      "loss": 0.7837,
      "step": 53600
    },
    {
      "epoch": 1.321846153846154,
      "grad_norm": 5.633899211883545,
      "learning_rate": 1.8652255964699554e-05,
      "loss": 0.7929,
      "step": 53700
    },
    {
      "epoch": 1.3243076923076922,
      "grad_norm": 7.982422351837158,
      "learning_rate": 1.8624905412674244e-05,
      "loss": 0.7754,
      "step": 53800
    },
    {
      "epoch": 1.3267692307692307,
      "grad_norm": 9.412178993225098,
      "learning_rate": 1.8597554860648938e-05,
      "loss": 0.7807,
      "step": 53900
    },
    {
      "epoch": 1.3292307692307692,
      "grad_norm": 6.288997173309326,
      "learning_rate": 1.857020430862363e-05,
      "loss": 0.7551,
      "step": 54000
    },
    {
      "epoch": 1.3316923076923077,
      "grad_norm": 4.892038345336914,
      "learning_rate": 1.8542853756598322e-05,
      "loss": 0.7671,
      "step": 54100
    },
    {
      "epoch": 1.3341538461538462,
      "grad_norm": 4.395851135253906,
      "learning_rate": 1.8515503204573013e-05,
      "loss": 0.789,
      "step": 54200
    },
    {
      "epoch": 1.3366153846153845,
      "grad_norm": 5.194668292999268,
      "learning_rate": 1.8488152652547707e-05,
      "loss": 0.7688,
      "step": 54300
    },
    {
      "epoch": 1.339076923076923,
      "grad_norm": 11.549970626831055,
      "learning_rate": 1.8460802100522397e-05,
      "loss": 0.7626,
      "step": 54400
    },
    {
      "epoch": 1.3415384615384616,
      "grad_norm": 6.07373571395874,
      "learning_rate": 1.8433451548497088e-05,
      "loss": 0.7752,
      "step": 54500
    },
    {
      "epoch": 1.3439999999999999,
      "grad_norm": 5.698633193969727,
      "learning_rate": 1.8406100996471782e-05,
      "loss": 0.7721,
      "step": 54600
    },
    {
      "epoch": 1.3464615384615386,
      "grad_norm": 5.760124683380127,
      "learning_rate": 1.837875044444647e-05,
      "loss": 0.7397,
      "step": 54700
    },
    {
      "epoch": 1.348923076923077,
      "grad_norm": 6.819797039031982,
      "learning_rate": 1.8351399892421163e-05,
      "loss": 0.7386,
      "step": 54800
    },
    {
      "epoch": 1.3513846153846154,
      "grad_norm": 8.591835975646973,
      "learning_rate": 1.8324049340395853e-05,
      "loss": 0.7482,
      "step": 54900
    },
    {
      "epoch": 1.353846153846154,
      "grad_norm": 7.286787033081055,
      "learning_rate": 1.8296698788370544e-05,
      "loss": 0.7519,
      "step": 55000
    },
    {
      "epoch": 1.3563076923076922,
      "grad_norm": 6.48502779006958,
      "learning_rate": 1.8269348236345238e-05,
      "loss": 0.8211,
      "step": 55100
    },
    {
      "epoch": 1.3587692307692307,
      "grad_norm": 5.751131534576416,
      "learning_rate": 1.8241997684319928e-05,
      "loss": 0.7744,
      "step": 55200
    },
    {
      "epoch": 1.3612307692307692,
      "grad_norm": 7.650498867034912,
      "learning_rate": 1.8214647132294622e-05,
      "loss": 0.7389,
      "step": 55300
    },
    {
      "epoch": 1.3636923076923078,
      "grad_norm": 5.560533046722412,
      "learning_rate": 1.8187296580269313e-05,
      "loss": 0.7833,
      "step": 55400
    },
    {
      "epoch": 1.3661538461538463,
      "grad_norm": 4.154871463775635,
      "learning_rate": 1.8159946028244003e-05,
      "loss": 0.763,
      "step": 55500
    },
    {
      "epoch": 1.3686153846153846,
      "grad_norm": 7.95776891708374,
      "learning_rate": 1.8132595476218697e-05,
      "loss": 0.7256,
      "step": 55600
    },
    {
      "epoch": 1.371076923076923,
      "grad_norm": 9.96369743347168,
      "learning_rate": 1.8105244924193388e-05,
      "loss": 0.7289,
      "step": 55700
    },
    {
      "epoch": 1.3735384615384616,
      "grad_norm": 6.054591178894043,
      "learning_rate": 1.807789437216808e-05,
      "loss": 0.7317,
      "step": 55800
    },
    {
      "epoch": 1.376,
      "grad_norm": 7.589120388031006,
      "learning_rate": 1.8050543820142772e-05,
      "loss": 0.7687,
      "step": 55900
    },
    {
      "epoch": 1.3784615384615384,
      "grad_norm": 5.583488464355469,
      "learning_rate": 1.802319326811746e-05,
      "loss": 0.7664,
      "step": 56000
    },
    {
      "epoch": 1.380923076923077,
      "grad_norm": 8.529067039489746,
      "learning_rate": 1.7995842716092153e-05,
      "loss": 0.757,
      "step": 56100
    },
    {
      "epoch": 1.3833846153846154,
      "grad_norm": 6.698815822601318,
      "learning_rate": 1.7968492164066844e-05,
      "loss": 0.7711,
      "step": 56200
    },
    {
      "epoch": 1.385846153846154,
      "grad_norm": 5.6908416748046875,
      "learning_rate": 1.7941141612041537e-05,
      "loss": 0.7919,
      "step": 56300
    },
    {
      "epoch": 1.3883076923076922,
      "grad_norm": 6.365900993347168,
      "learning_rate": 1.7914064565536482e-05,
      "loss": 0.7832,
      "step": 56400
    },
    {
      "epoch": 1.3907692307692308,
      "grad_norm": 4.5490546226501465,
      "learning_rate": 1.7886714013511173e-05,
      "loss": 0.7557,
      "step": 56500
    },
    {
      "epoch": 1.3932307692307693,
      "grad_norm": 4.297024250030518,
      "learning_rate": 1.7859363461485863e-05,
      "loss": 0.7685,
      "step": 56600
    },
    {
      "epoch": 1.3956923076923076,
      "grad_norm": 7.44199800491333,
      "learning_rate": 1.7832012909460557e-05,
      "loss": 0.7588,
      "step": 56700
    },
    {
      "epoch": 1.398153846153846,
      "grad_norm": 5.373941421508789,
      "learning_rate": 1.7804662357435248e-05,
      "loss": 0.736,
      "step": 56800
    },
    {
      "epoch": 1.4006153846153846,
      "grad_norm": 6.646990776062012,
      "learning_rate": 1.7777311805409938e-05,
      "loss": 0.8037,
      "step": 56900
    },
    {
      "epoch": 1.403076923076923,
      "grad_norm": 8.22325611114502,
      "learning_rate": 1.7749961253384632e-05,
      "loss": 0.7867,
      "step": 57000
    },
    {
      "epoch": 1.4055384615384616,
      "grad_norm": 6.289400577545166,
      "learning_rate": 1.7722610701359323e-05,
      "loss": 0.777,
      "step": 57100
    },
    {
      "epoch": 1.408,
      "grad_norm": 7.160399913787842,
      "learning_rate": 1.7695260149334016e-05,
      "loss": 0.784,
      "step": 57200
    },
    {
      "epoch": 1.4104615384615384,
      "grad_norm": 3.957336664199829,
      "learning_rate": 1.7667909597308707e-05,
      "loss": 0.7854,
      "step": 57300
    },
    {
      "epoch": 1.412923076923077,
      "grad_norm": 4.110988616943359,
      "learning_rate": 1.7640559045283397e-05,
      "loss": 0.7391,
      "step": 57400
    },
    {
      "epoch": 1.4153846153846155,
      "grad_norm": 5.894679546356201,
      "learning_rate": 1.761320849325809e-05,
      "loss": 0.7318,
      "step": 57500
    },
    {
      "epoch": 1.417846153846154,
      "grad_norm": 10.522054672241211,
      "learning_rate": 1.7585857941232782e-05,
      "loss": 0.7773,
      "step": 57600
    },
    {
      "epoch": 1.4203076923076923,
      "grad_norm": 5.546632766723633,
      "learning_rate": 1.7558507389207472e-05,
      "loss": 0.7717,
      "step": 57700
    },
    {
      "epoch": 1.4227692307692308,
      "grad_norm": 7.950186252593994,
      "learning_rate": 1.7531156837182163e-05,
      "loss": 0.7321,
      "step": 57800
    },
    {
      "epoch": 1.4252307692307693,
      "grad_norm": 6.798851013183594,
      "learning_rate": 1.7503806285156853e-05,
      "loss": 0.7537,
      "step": 57900
    },
    {
      "epoch": 1.4276923076923076,
      "grad_norm": 8.503490447998047,
      "learning_rate": 1.7476455733131547e-05,
      "loss": 0.7416,
      "step": 58000
    },
    {
      "epoch": 1.430153846153846,
      "grad_norm": 4.597592353820801,
      "learning_rate": 1.7449105181106238e-05,
      "loss": 0.7752,
      "step": 58100
    },
    {
      "epoch": 1.4326153846153846,
      "grad_norm": 7.152798175811768,
      "learning_rate": 1.7421754629080932e-05,
      "loss": 0.7811,
      "step": 58200
    },
    {
      "epoch": 1.4350769230769231,
      "grad_norm": 5.604127883911133,
      "learning_rate": 1.7394404077055622e-05,
      "loss": 0.7655,
      "step": 58300
    },
    {
      "epoch": 1.4375384615384617,
      "grad_norm": 5.860188007354736,
      "learning_rate": 1.7367053525030313e-05,
      "loss": 0.7534,
      "step": 58400
    },
    {
      "epoch": 1.44,
      "grad_norm": 4.358096122741699,
      "learning_rate": 1.7339702973005007e-05,
      "loss": 0.7804,
      "step": 58500
    },
    {
      "epoch": 1.4424615384615385,
      "grad_norm": 8.044473648071289,
      "learning_rate": 1.7312352420979697e-05,
      "loss": 0.7365,
      "step": 58600
    },
    {
      "epoch": 1.444923076923077,
      "grad_norm": 6.497180461883545,
      "learning_rate": 1.728500186895439e-05,
      "loss": 0.7301,
      "step": 58700
    },
    {
      "epoch": 1.4473846153846153,
      "grad_norm": 5.65379524230957,
      "learning_rate": 1.725765131692908e-05,
      "loss": 0.7614,
      "step": 58800
    },
    {
      "epoch": 1.4498461538461538,
      "grad_norm": 7.418606281280518,
      "learning_rate": 1.7230574270424026e-05,
      "loss": 0.7617,
      "step": 58900
    },
    {
      "epoch": 1.4523076923076923,
      "grad_norm": 4.553598880767822,
      "learning_rate": 1.7203223718398717e-05,
      "loss": 0.76,
      "step": 59000
    },
    {
      "epoch": 1.4547692307692308,
      "grad_norm": 6.594920635223389,
      "learning_rate": 1.717587316637341e-05,
      "loss": 0.7911,
      "step": 59100
    },
    {
      "epoch": 1.4572307692307693,
      "grad_norm": 4.907206058502197,
      "learning_rate": 1.71485226143481e-05,
      "loss": 0.778,
      "step": 59200
    },
    {
      "epoch": 1.4596923076923076,
      "grad_norm": 5.846840858459473,
      "learning_rate": 1.7121172062322792e-05,
      "loss": 0.7712,
      "step": 59300
    },
    {
      "epoch": 1.4621538461538461,
      "grad_norm": 19.041770935058594,
      "learning_rate": 1.7093821510297486e-05,
      "loss": 0.737,
      "step": 59400
    },
    {
      "epoch": 1.4646153846153847,
      "grad_norm": 8.699874877929688,
      "learning_rate": 1.7066470958272173e-05,
      "loss": 0.7598,
      "step": 59500
    },
    {
      "epoch": 1.467076923076923,
      "grad_norm": 5.554555416107178,
      "learning_rate": 1.7039120406246867e-05,
      "loss": 0.7903,
      "step": 59600
    },
    {
      "epoch": 1.4695384615384615,
      "grad_norm": 7.664175033569336,
      "learning_rate": 1.7011769854221557e-05,
      "loss": 0.7789,
      "step": 59700
    },
    {
      "epoch": 1.472,
      "grad_norm": 4.163649082183838,
      "learning_rate": 1.6984419302196248e-05,
      "loss": 0.7829,
      "step": 59800
    },
    {
      "epoch": 1.4744615384615385,
      "grad_norm": 9.667276382446289,
      "learning_rate": 1.695706875017094e-05,
      "loss": 0.7637,
      "step": 59900
    },
    {
      "epoch": 1.476923076923077,
      "grad_norm": 7.550189018249512,
      "learning_rate": 1.6929718198145632e-05,
      "loss": 0.8052,
      "step": 60000
    },
    {
      "epoch": 1.4793846153846153,
      "grad_norm": 5.5117950439453125,
      "learning_rate": 1.6902367646120326e-05,
      "loss": 0.7617,
      "step": 60100
    },
    {
      "epoch": 1.4818461538461538,
      "grad_norm": 11.527958869934082,
      "learning_rate": 1.6875017094095017e-05,
      "loss": 0.7721,
      "step": 60200
    },
    {
      "epoch": 1.4843076923076923,
      "grad_norm": 5.598045825958252,
      "learning_rate": 1.6847666542069707e-05,
      "loss": 0.7995,
      "step": 60300
    },
    {
      "epoch": 1.4867692307692308,
      "grad_norm": 8.046021461486816,
      "learning_rate": 1.68203159900444e-05,
      "loss": 0.7701,
      "step": 60400
    },
    {
      "epoch": 1.4892307692307694,
      "grad_norm": 6.563763618469238,
      "learning_rate": 1.679296543801909e-05,
      "loss": 0.7698,
      "step": 60500
    },
    {
      "epoch": 1.4916923076923077,
      "grad_norm": 6.501736164093018,
      "learning_rate": 1.6765614885993785e-05,
      "loss": 0.7801,
      "step": 60600
    },
    {
      "epoch": 1.4941538461538462,
      "grad_norm": 4.66300630569458,
      "learning_rate": 1.6738264333968476e-05,
      "loss": 0.751,
      "step": 60700
    },
    {
      "epoch": 1.4966153846153847,
      "grad_norm": 3.665581464767456,
      "learning_rate": 1.6710913781943163e-05,
      "loss": 0.7469,
      "step": 60800
    },
    {
      "epoch": 1.499076923076923,
      "grad_norm": 5.7825188636779785,
      "learning_rate": 1.6683563229917857e-05,
      "loss": 0.7874,
      "step": 60900
    },
    {
      "epoch": 1.5015384615384615,
      "grad_norm": 8.84267807006836,
      "learning_rate": 1.6656212677892547e-05,
      "loss": 0.7526,
      "step": 61000
    },
    {
      "epoch": 1.504,
      "grad_norm": 10.356005668640137,
      "learning_rate": 1.662886212586724e-05,
      "loss": 0.747,
      "step": 61100
    },
    {
      "epoch": 1.5064615384615383,
      "grad_norm": 5.029629707336426,
      "learning_rate": 1.6601785079362183e-05,
      "loss": 0.7659,
      "step": 61200
    },
    {
      "epoch": 1.508923076923077,
      "grad_norm": 5.8886542320251465,
      "learning_rate": 1.6574434527336877e-05,
      "loss": 0.746,
      "step": 61300
    },
    {
      "epoch": 1.5113846153846153,
      "grad_norm": 5.559134483337402,
      "learning_rate": 1.6547083975311567e-05,
      "loss": 0.7698,
      "step": 61400
    },
    {
      "epoch": 1.5138461538461538,
      "grad_norm": 4.517293930053711,
      "learning_rate": 1.651973342328626e-05,
      "loss": 0.79,
      "step": 61500
    },
    {
      "epoch": 1.5163076923076924,
      "grad_norm": 5.389591217041016,
      "learning_rate": 1.649238287126095e-05,
      "loss": 0.7734,
      "step": 61600
    },
    {
      "epoch": 1.5187692307692306,
      "grad_norm": 6.396076202392578,
      "learning_rate": 1.6465032319235642e-05,
      "loss": 0.7406,
      "step": 61700
    },
    {
      "epoch": 1.5212307692307694,
      "grad_norm": 6.358200550079346,
      "learning_rate": 1.6437681767210336e-05,
      "loss": 0.7509,
      "step": 61800
    },
    {
      "epoch": 1.5236923076923077,
      "grad_norm": 6.737220764160156,
      "learning_rate": 1.6410331215185026e-05,
      "loss": 0.7501,
      "step": 61900
    },
    {
      "epoch": 1.5261538461538462,
      "grad_norm": 5.747537612915039,
      "learning_rate": 1.638298066315972e-05,
      "loss": 0.7596,
      "step": 62000
    },
    {
      "epoch": 1.5286153846153847,
      "grad_norm": 6.496221542358398,
      "learning_rate": 1.635563011113441e-05,
      "loss": 0.7875,
      "step": 62100
    },
    {
      "epoch": 1.531076923076923,
      "grad_norm": 9.971570014953613,
      "learning_rate": 1.63282795591091e-05,
      "loss": 0.7387,
      "step": 62200
    },
    {
      "epoch": 1.5335384615384615,
      "grad_norm": 6.496131896972656,
      "learning_rate": 1.6300929007083795e-05,
      "loss": 0.7675,
      "step": 62300
    },
    {
      "epoch": 1.536,
      "grad_norm": 5.22841739654541,
      "learning_rate": 1.6273578455058486e-05,
      "loss": 0.7615,
      "step": 62400
    },
    {
      "epoch": 1.5384615384615383,
      "grad_norm": 3.9404397010803223,
      "learning_rate": 1.6246227903033176e-05,
      "loss": 0.786,
      "step": 62500
    },
    {
      "epoch": 1.540923076923077,
      "grad_norm": 6.4037275314331055,
      "learning_rate": 1.6218877351007867e-05,
      "loss": 0.7965,
      "step": 62600
    },
    {
      "epoch": 1.5433846153846154,
      "grad_norm": 4.918456554412842,
      "learning_rate": 1.6191526798982557e-05,
      "loss": 0.7837,
      "step": 62700
    },
    {
      "epoch": 1.5458461538461539,
      "grad_norm": 3.2041614055633545,
      "learning_rate": 1.616417624695725e-05,
      "loss": 0.7203,
      "step": 62800
    },
    {
      "epoch": 1.5483076923076924,
      "grad_norm": 4.449837684631348,
      "learning_rate": 1.6136825694931942e-05,
      "loss": 0.7667,
      "step": 62900
    },
    {
      "epoch": 1.5507692307692307,
      "grad_norm": 4.144417762756348,
      "learning_rate": 1.6109475142906636e-05,
      "loss": 0.7536,
      "step": 63000
    },
    {
      "epoch": 1.5532307692307692,
      "grad_norm": 8.25145149230957,
      "learning_rate": 1.6082124590881326e-05,
      "loss": 0.7762,
      "step": 63100
    },
    {
      "epoch": 1.5556923076923077,
      "grad_norm": 5.9251251220703125,
      "learning_rate": 1.6054774038856017e-05,
      "loss": 0.7572,
      "step": 63200
    },
    {
      "epoch": 1.558153846153846,
      "grad_norm": 8.995305061340332,
      "learning_rate": 1.602742348683071e-05,
      "loss": 0.7542,
      "step": 63300
    },
    {
      "epoch": 1.5606153846153847,
      "grad_norm": 5.885872840881348,
      "learning_rate": 1.60000729348054e-05,
      "loss": 0.7849,
      "step": 63400
    },
    {
      "epoch": 1.563076923076923,
      "grad_norm": 5.2777018547058105,
      "learning_rate": 1.5972722382780095e-05,
      "loss": 0.7284,
      "step": 63500
    },
    {
      "epoch": 1.5655384615384615,
      "grad_norm": 4.664752960205078,
      "learning_rate": 1.5945371830754785e-05,
      "loss": 0.7772,
      "step": 63600
    },
    {
      "epoch": 1.568,
      "grad_norm": 6.962779998779297,
      "learning_rate": 1.5918021278729476e-05,
      "loss": 0.7274,
      "step": 63700
    },
    {
      "epoch": 1.5704615384615384,
      "grad_norm": 5.817875385284424,
      "learning_rate": 1.5890670726704167e-05,
      "loss": 0.7913,
      "step": 63800
    },
    {
      "epoch": 1.5729230769230769,
      "grad_norm": 4.662332057952881,
      "learning_rate": 1.5863320174678857e-05,
      "loss": 0.776,
      "step": 63900
    },
    {
      "epoch": 1.5753846153846154,
      "grad_norm": 5.550382137298584,
      "learning_rate": 1.583596962265355e-05,
      "loss": 0.7592,
      "step": 64000
    },
    {
      "epoch": 1.577846153846154,
      "grad_norm": 5.7948479652404785,
      "learning_rate": 1.580861907062824e-05,
      "loss": 0.7827,
      "step": 64100
    },
    {
      "epoch": 1.5803076923076924,
      "grad_norm": 10.143084526062012,
      "learning_rate": 1.5781268518602935e-05,
      "loss": 0.7526,
      "step": 64200
    },
    {
      "epoch": 1.5827692307692307,
      "grad_norm": 7.851871967315674,
      "learning_rate": 1.5753917966577626e-05,
      "loss": 0.7793,
      "step": 64300
    },
    {
      "epoch": 1.5852307692307692,
      "grad_norm": 6.0110039710998535,
      "learning_rate": 1.5726567414552316e-05,
      "loss": 0.7478,
      "step": 64400
    },
    {
      "epoch": 1.5876923076923077,
      "grad_norm": 11.768996238708496,
      "learning_rate": 1.569921686252701e-05,
      "loss": 0.7479,
      "step": 64500
    },
    {
      "epoch": 1.590153846153846,
      "grad_norm": 4.170041561126709,
      "learning_rate": 1.56718663105017e-05,
      "loss": 0.7365,
      "step": 64600
    },
    {
      "epoch": 1.5926153846153848,
      "grad_norm": 6.7291340827941895,
      "learning_rate": 1.5644515758476395e-05,
      "loss": 0.7517,
      "step": 64700
    },
    {
      "epoch": 1.595076923076923,
      "grad_norm": 6.77694845199585,
      "learning_rate": 1.5617165206451085e-05,
      "loss": 0.8069,
      "step": 64800
    },
    {
      "epoch": 1.5975384615384616,
      "grad_norm": 5.772519588470459,
      "learning_rate": 1.5589814654425776e-05,
      "loss": 0.7391,
      "step": 64900
    },
    {
      "epoch": 1.6,
      "grad_norm": 4.744128227233887,
      "learning_rate": 1.556246410240047e-05,
      "loss": 0.7615,
      "step": 65000
    },
    {
      "epoch": 1.6024615384615384,
      "grad_norm": 4.3816962242126465,
      "learning_rate": 1.5535113550375157e-05,
      "loss": 0.7761,
      "step": 65100
    },
    {
      "epoch": 1.604923076923077,
      "grad_norm": 5.548408031463623,
      "learning_rate": 1.5508036503870105e-05,
      "loss": 0.7748,
      "step": 65200
    },
    {
      "epoch": 1.6073846153846154,
      "grad_norm": 4.44359827041626,
      "learning_rate": 1.548095945736505e-05,
      "loss": 0.7419,
      "step": 65300
    },
    {
      "epoch": 1.6098461538461537,
      "grad_norm": 5.9951066970825195,
      "learning_rate": 1.545360890533974e-05,
      "loss": 0.7237,
      "step": 65400
    },
    {
      "epoch": 1.6123076923076924,
      "grad_norm": 6.043390274047852,
      "learning_rate": 1.542625835331443e-05,
      "loss": 0.7661,
      "step": 65500
    },
    {
      "epoch": 1.6147692307692307,
      "grad_norm": 6.178686141967773,
      "learning_rate": 1.5398907801289124e-05,
      "loss": 0.7583,
      "step": 65600
    },
    {
      "epoch": 1.6172307692307692,
      "grad_norm": 7.958882808685303,
      "learning_rate": 1.5371557249263815e-05,
      "loss": 0.7747,
      "step": 65700
    },
    {
      "epoch": 1.6196923076923078,
      "grad_norm": 6.443968772888184,
      "learning_rate": 1.534420669723851e-05,
      "loss": 0.8101,
      "step": 65800
    },
    {
      "epoch": 1.622153846153846,
      "grad_norm": 7.3918890953063965,
      "learning_rate": 1.53168561452132e-05,
      "loss": 0.7593,
      "step": 65900
    },
    {
      "epoch": 1.6246153846153846,
      "grad_norm": 5.235543727874756,
      "learning_rate": 1.5289505593187886e-05,
      "loss": 0.7816,
      "step": 66000
    },
    {
      "epoch": 1.627076923076923,
      "grad_norm": 7.48392915725708,
      "learning_rate": 1.526215504116258e-05,
      "loss": 0.7733,
      "step": 66100
    },
    {
      "epoch": 1.6295384615384614,
      "grad_norm": 5.909783840179443,
      "learning_rate": 1.5234804489137273e-05,
      "loss": 0.753,
      "step": 66200
    },
    {
      "epoch": 1.6320000000000001,
      "grad_norm": 5.685108184814453,
      "learning_rate": 1.5207453937111963e-05,
      "loss": 0.7442,
      "step": 66300
    },
    {
      "epoch": 1.6344615384615384,
      "grad_norm": 5.4326348304748535,
      "learning_rate": 1.5180103385086655e-05,
      "loss": 0.7507,
      "step": 66400
    },
    {
      "epoch": 1.636923076923077,
      "grad_norm": 6.29379940032959,
      "learning_rate": 1.5152752833061348e-05,
      "loss": 0.758,
      "step": 66500
    },
    {
      "epoch": 1.6393846153846154,
      "grad_norm": 4.8902692794799805,
      "learning_rate": 1.512540228103604e-05,
      "loss": 0.7729,
      "step": 66600
    },
    {
      "epoch": 1.6418461538461537,
      "grad_norm": 3.8700003623962402,
      "learning_rate": 1.5098051729010732e-05,
      "loss": 0.7472,
      "step": 66700
    },
    {
      "epoch": 1.6443076923076925,
      "grad_norm": 8.820777893066406,
      "learning_rate": 1.5070701176985422e-05,
      "loss": 0.775,
      "step": 66800
    },
    {
      "epoch": 1.6467692307692308,
      "grad_norm": 9.253565788269043,
      "learning_rate": 1.5043350624960115e-05,
      "loss": 0.7789,
      "step": 66900
    },
    {
      "epoch": 1.6492307692307693,
      "grad_norm": 7.638382911682129,
      "learning_rate": 1.5016000072934807e-05,
      "loss": 0.7116,
      "step": 67000
    },
    {
      "epoch": 1.6516923076923078,
      "grad_norm": 6.980747222900391,
      "learning_rate": 1.4988649520909497e-05,
      "loss": 0.7869,
      "step": 67100
    },
    {
      "epoch": 1.654153846153846,
      "grad_norm": 4.042535305023193,
      "learning_rate": 1.496129896888419e-05,
      "loss": 0.7794,
      "step": 67200
    },
    {
      "epoch": 1.6566153846153846,
      "grad_norm": 4.598426342010498,
      "learning_rate": 1.493394841685888e-05,
      "loss": 0.7825,
      "step": 67300
    },
    {
      "epoch": 1.6590769230769231,
      "grad_norm": 6.093238353729248,
      "learning_rate": 1.4906597864833572e-05,
      "loss": 0.7494,
      "step": 67400
    },
    {
      "epoch": 1.6615384615384614,
      "grad_norm": 5.386308670043945,
      "learning_rate": 1.4879247312808265e-05,
      "loss": 0.7946,
      "step": 67500
    },
    {
      "epoch": 1.6640000000000001,
      "grad_norm": 7.867615699768066,
      "learning_rate": 1.4851896760782955e-05,
      "loss": 0.7566,
      "step": 67600
    },
    {
      "epoch": 1.6664615384615384,
      "grad_norm": 7.901984214782715,
      "learning_rate": 1.4824546208757647e-05,
      "loss": 0.7835,
      "step": 67700
    },
    {
      "epoch": 1.668923076923077,
      "grad_norm": 5.685085773468018,
      "learning_rate": 1.4797195656732338e-05,
      "loss": 0.7771,
      "step": 67800
    },
    {
      "epoch": 1.6713846153846155,
      "grad_norm": 5.5353851318359375,
      "learning_rate": 1.476984510470703e-05,
      "loss": 0.7562,
      "step": 67900
    },
    {
      "epoch": 1.6738461538461538,
      "grad_norm": 4.616561412811279,
      "learning_rate": 1.4742494552681722e-05,
      "loss": 0.7854,
      "step": 68000
    },
    {
      "epoch": 1.6763076923076923,
      "grad_norm": 4.651104927062988,
      "learning_rate": 1.4715144000656414e-05,
      "loss": 0.8139,
      "step": 68100
    },
    {
      "epoch": 1.6787692307692308,
      "grad_norm": 9.563836097717285,
      "learning_rate": 1.4687793448631107e-05,
      "loss": 0.7711,
      "step": 68200
    },
    {
      "epoch": 1.681230769230769,
      "grad_norm": 6.582801818847656,
      "learning_rate": 1.4660442896605795e-05,
      "loss": 0.7578,
      "step": 68300
    },
    {
      "epoch": 1.6836923076923078,
      "grad_norm": 3.241395950317383,
      "learning_rate": 1.4633092344580488e-05,
      "loss": 0.757,
      "step": 68400
    },
    {
      "epoch": 1.6861538461538461,
      "grad_norm": 5.504138946533203,
      "learning_rate": 1.460574179255518e-05,
      "loss": 0.7249,
      "step": 68500
    },
    {
      "epoch": 1.6886153846153846,
      "grad_norm": 9.619511604309082,
      "learning_rate": 1.4578391240529872e-05,
      "loss": 0.7727,
      "step": 68600
    },
    {
      "epoch": 1.6910769230769231,
      "grad_norm": 3.189613103866577,
      "learning_rate": 1.4551040688504564e-05,
      "loss": 0.7655,
      "step": 68700
    },
    {
      "epoch": 1.6935384615384614,
      "grad_norm": 8.063800811767578,
      "learning_rate": 1.4523690136479257e-05,
      "loss": 0.7868,
      "step": 68800
    },
    {
      "epoch": 1.696,
      "grad_norm": 4.194296836853027,
      "learning_rate": 1.4496339584453945e-05,
      "loss": 0.7512,
      "step": 68900
    },
    {
      "epoch": 1.6984615384615385,
      "grad_norm": 6.399143218994141,
      "learning_rate": 1.4468989032428638e-05,
      "loss": 0.7657,
      "step": 69000
    },
    {
      "epoch": 1.7009230769230768,
      "grad_norm": 6.588242530822754,
      "learning_rate": 1.444163848040333e-05,
      "loss": 0.7588,
      "step": 69100
    },
    {
      "epoch": 1.7033846153846155,
      "grad_norm": 7.5962724685668945,
      "learning_rate": 1.4414287928378022e-05,
      "loss": 0.7817,
      "step": 69200
    },
    {
      "epoch": 1.7058461538461538,
      "grad_norm": 8.314080238342285,
      "learning_rate": 1.4386937376352714e-05,
      "loss": 0.7566,
      "step": 69300
    },
    {
      "epoch": 1.7083076923076923,
      "grad_norm": 4.0712127685546875,
      "learning_rate": 1.4360133835367911e-05,
      "loss": 0.7731,
      "step": 69400
    },
    {
      "epoch": 1.7107692307692308,
      "grad_norm": 5.519340991973877,
      "learning_rate": 1.4332783283342602e-05,
      "loss": 0.751,
      "step": 69500
    },
    {
      "epoch": 1.7132307692307691,
      "grad_norm": 10.896696090698242,
      "learning_rate": 1.4305432731317294e-05,
      "loss": 0.7518,
      "step": 69600
    },
    {
      "epoch": 1.7156923076923078,
      "grad_norm": 9.689898490905762,
      "learning_rate": 1.4278082179291986e-05,
      "loss": 0.7418,
      "step": 69700
    },
    {
      "epoch": 1.7181538461538461,
      "grad_norm": 6.15034294128418,
      "learning_rate": 1.4250731627266677e-05,
      "loss": 0.7511,
      "step": 69800
    },
    {
      "epoch": 1.7206153846153847,
      "grad_norm": 6.747042179107666,
      "learning_rate": 1.4223381075241369e-05,
      "loss": 0.7857,
      "step": 69900
    },
    {
      "epoch": 1.7230769230769232,
      "grad_norm": 3.5707318782806396,
      "learning_rate": 1.4196030523216061e-05,
      "loss": 0.766,
      "step": 70000
    },
    {
      "epoch": 1.7255384615384615,
      "grad_norm": 4.396826267242432,
      "learning_rate": 1.4168679971190752e-05,
      "loss": 0.7604,
      "step": 70100
    },
    {
      "epoch": 1.728,
      "grad_norm": 7.988900661468506,
      "learning_rate": 1.4141329419165444e-05,
      "loss": 0.7328,
      "step": 70200
    },
    {
      "epoch": 1.7304615384615385,
      "grad_norm": 8.147391319274902,
      "learning_rate": 1.4113978867140136e-05,
      "loss": 0.7973,
      "step": 70300
    },
    {
      "epoch": 1.7329230769230768,
      "grad_norm": 6.017682075500488,
      "learning_rate": 1.4086628315114828e-05,
      "loss": 0.7884,
      "step": 70400
    },
    {
      "epoch": 1.7353846153846155,
      "grad_norm": 7.9532856941223145,
      "learning_rate": 1.4059277763089519e-05,
      "loss": 0.7851,
      "step": 70500
    },
    {
      "epoch": 1.7378461538461538,
      "grad_norm": 5.674332141876221,
      "learning_rate": 1.403192721106421e-05,
      "loss": 0.7661,
      "step": 70600
    },
    {
      "epoch": 1.7403076923076923,
      "grad_norm": 9.092019081115723,
      "learning_rate": 1.4004576659038902e-05,
      "loss": 0.7338,
      "step": 70700
    },
    {
      "epoch": 1.7427692307692308,
      "grad_norm": 6.400669097900391,
      "learning_rate": 1.3977226107013594e-05,
      "loss": 0.7341,
      "step": 70800
    },
    {
      "epoch": 1.7452307692307691,
      "grad_norm": 6.73065185546875,
      "learning_rate": 1.3949875554988286e-05,
      "loss": 0.7757,
      "step": 70900
    },
    {
      "epoch": 1.7476923076923077,
      "grad_norm": 4.317751407623291,
      "learning_rate": 1.3922525002962978e-05,
      "loss": 0.7758,
      "step": 71000
    },
    {
      "epoch": 1.7501538461538462,
      "grad_norm": 6.074052810668945,
      "learning_rate": 1.3895174450937667e-05,
      "loss": 0.7446,
      "step": 71100
    },
    {
      "epoch": 1.7526153846153845,
      "grad_norm": 5.217020034790039,
      "learning_rate": 1.386782389891236e-05,
      "loss": 0.7454,
      "step": 71200
    },
    {
      "epoch": 1.7550769230769232,
      "grad_norm": 6.914371967315674,
      "learning_rate": 1.3840473346887051e-05,
      "loss": 0.7926,
      "step": 71300
    },
    {
      "epoch": 1.7575384615384615,
      "grad_norm": 3.959766149520874,
      "learning_rate": 1.3813122794861744e-05,
      "loss": 0.7469,
      "step": 71400
    },
    {
      "epoch": 1.76,
      "grad_norm": 6.793583869934082,
      "learning_rate": 1.3785772242836436e-05,
      "loss": 0.7689,
      "step": 71500
    },
    {
      "epoch": 1.7624615384615385,
      "grad_norm": 4.505187511444092,
      "learning_rate": 1.3758421690811126e-05,
      "loss": 0.7472,
      "step": 71600
    },
    {
      "epoch": 1.7649230769230768,
      "grad_norm": 5.656733989715576,
      "learning_rate": 1.3731071138785819e-05,
      "loss": 0.7504,
      "step": 71700
    },
    {
      "epoch": 1.7673846153846153,
      "grad_norm": 7.4833292961120605,
      "learning_rate": 1.3703720586760509e-05,
      "loss": 0.7804,
      "step": 71800
    },
    {
      "epoch": 1.7698461538461538,
      "grad_norm": 3.7580173015594482,
      "learning_rate": 1.3676370034735201e-05,
      "loss": 0.7423,
      "step": 71900
    },
    {
      "epoch": 1.7723076923076924,
      "grad_norm": 7.598771572113037,
      "learning_rate": 1.3649019482709893e-05,
      "loss": 0.7952,
      "step": 72000
    },
    {
      "epoch": 1.7747692307692309,
      "grad_norm": 6.299998760223389,
      "learning_rate": 1.3621668930684586e-05,
      "loss": 0.76,
      "step": 72100
    },
    {
      "epoch": 1.7772307692307692,
      "grad_norm": 4.781939506530762,
      "learning_rate": 1.3594318378659276e-05,
      "loss": 0.7607,
      "step": 72200
    },
    {
      "epoch": 1.7796923076923077,
      "grad_norm": 8.697969436645508,
      "learning_rate": 1.3566967826633968e-05,
      "loss": 0.7372,
      "step": 72300
    },
    {
      "epoch": 1.7821538461538462,
      "grad_norm": 7.305964469909668,
      "learning_rate": 1.3539617274608659e-05,
      "loss": 0.7534,
      "step": 72400
    },
    {
      "epoch": 1.7846153846153845,
      "grad_norm": 8.815099716186523,
      "learning_rate": 1.3512266722583351e-05,
      "loss": 0.7564,
      "step": 72500
    },
    {
      "epoch": 1.7870769230769232,
      "grad_norm": 6.685750961303711,
      "learning_rate": 1.3484916170558043e-05,
      "loss": 0.7725,
      "step": 72600
    },
    {
      "epoch": 1.7895384615384615,
      "grad_norm": 6.758480072021484,
      "learning_rate": 1.3457565618532734e-05,
      "loss": 0.7816,
      "step": 72700
    },
    {
      "epoch": 1.792,
      "grad_norm": 5.229106903076172,
      "learning_rate": 1.3430215066507426e-05,
      "loss": 0.7674,
      "step": 72800
    },
    {
      "epoch": 1.7944615384615386,
      "grad_norm": 5.709878444671631,
      "learning_rate": 1.3402864514482118e-05,
      "loss": 0.7733,
      "step": 72900
    },
    {
      "epoch": 1.7969230769230768,
      "grad_norm": 5.935667514801025,
      "learning_rate": 1.3375513962456809e-05,
      "loss": 0.8004,
      "step": 73000
    },
    {
      "epoch": 1.7993846153846154,
      "grad_norm": 8.451641082763672,
      "learning_rate": 1.3348163410431501e-05,
      "loss": 0.7739,
      "step": 73100
    },
    {
      "epoch": 1.8018461538461539,
      "grad_norm": 5.273586750030518,
      "learning_rate": 1.3320812858406192e-05,
      "loss": 0.751,
      "step": 73200
    },
    {
      "epoch": 1.8043076923076922,
      "grad_norm": 9.419878005981445,
      "learning_rate": 1.3293462306380884e-05,
      "loss": 0.7719,
      "step": 73300
    },
    {
      "epoch": 1.806769230769231,
      "grad_norm": 7.318176746368408,
      "learning_rate": 1.3266111754355576e-05,
      "loss": 0.7863,
      "step": 73400
    },
    {
      "epoch": 1.8092307692307692,
      "grad_norm": 3.5912342071533203,
      "learning_rate": 1.3238761202330268e-05,
      "loss": 0.7826,
      "step": 73500
    },
    {
      "epoch": 1.8116923076923077,
      "grad_norm": 4.385746002197266,
      "learning_rate": 1.3211684155825211e-05,
      "loss": 0.7721,
      "step": 73600
    },
    {
      "epoch": 1.8141538461538462,
      "grad_norm": 4.866944313049316,
      "learning_rate": 1.3184333603799903e-05,
      "loss": 0.7517,
      "step": 73700
    },
    {
      "epoch": 1.8166153846153845,
      "grad_norm": 5.5814127922058105,
      "learning_rate": 1.3156983051774596e-05,
      "loss": 0.7713,
      "step": 73800
    },
    {
      "epoch": 1.819076923076923,
      "grad_norm": 6.944929122924805,
      "learning_rate": 1.3129632499749288e-05,
      "loss": 0.7704,
      "step": 73900
    },
    {
      "epoch": 1.8215384615384616,
      "grad_norm": 5.159255504608154,
      "learning_rate": 1.310228194772398e-05,
      "loss": 0.7858,
      "step": 74000
    },
    {
      "epoch": 1.8239999999999998,
      "grad_norm": 7.557092189788818,
      "learning_rate": 1.3074931395698669e-05,
      "loss": 0.7771,
      "step": 74100
    },
    {
      "epoch": 1.8264615384615386,
      "grad_norm": 6.481719970703125,
      "learning_rate": 1.3047580843673361e-05,
      "loss": 0.7716,
      "step": 74200
    },
    {
      "epoch": 1.8289230769230769,
      "grad_norm": 5.6846442222595215,
      "learning_rate": 1.3020230291648053e-05,
      "loss": 0.7524,
      "step": 74300
    },
    {
      "epoch": 1.8313846153846154,
      "grad_norm": 5.481753349304199,
      "learning_rate": 1.2992879739622745e-05,
      "loss": 0.7221,
      "step": 74400
    },
    {
      "epoch": 1.833846153846154,
      "grad_norm": 4.630505561828613,
      "learning_rate": 1.2965529187597438e-05,
      "loss": 0.7033,
      "step": 74500
    },
    {
      "epoch": 1.8363076923076922,
      "grad_norm": 6.364639759063721,
      "learning_rate": 1.2938178635572128e-05,
      "loss": 0.7189,
      "step": 74600
    },
    {
      "epoch": 1.838769230769231,
      "grad_norm": 5.684357643127441,
      "learning_rate": 1.291082808354682e-05,
      "loss": 0.7456,
      "step": 74700
    },
    {
      "epoch": 1.8412307692307692,
      "grad_norm": 4.011932849884033,
      "learning_rate": 1.2883477531521511e-05,
      "loss": 0.7982,
      "step": 74800
    },
    {
      "epoch": 1.8436923076923077,
      "grad_norm": 6.992441654205322,
      "learning_rate": 1.2856126979496203e-05,
      "loss": 0.7536,
      "step": 74900
    },
    {
      "epoch": 1.8461538461538463,
      "grad_norm": 7.366031646728516,
      "learning_rate": 1.2828776427470895e-05,
      "loss": 0.732,
      "step": 75000
    },
    {
      "epoch": 1.8486153846153845,
      "grad_norm": 5.786498069763184,
      "learning_rate": 1.2801425875445586e-05,
      "loss": 0.7736,
      "step": 75100
    },
    {
      "epoch": 1.851076923076923,
      "grad_norm": 6.67153263092041,
      "learning_rate": 1.2774075323420278e-05,
      "loss": 0.7654,
      "step": 75200
    },
    {
      "epoch": 1.8535384615384616,
      "grad_norm": 7.981317520141602,
      "learning_rate": 1.274672477139497e-05,
      "loss": 0.7806,
      "step": 75300
    },
    {
      "epoch": 1.8559999999999999,
      "grad_norm": 5.057671546936035,
      "learning_rate": 1.271937421936966e-05,
      "loss": 0.7426,
      "step": 75400
    },
    {
      "epoch": 1.8584615384615386,
      "grad_norm": 6.466870307922363,
      "learning_rate": 1.2692023667344353e-05,
      "loss": 0.7655,
      "step": 75500
    },
    {
      "epoch": 1.860923076923077,
      "grad_norm": 7.934910297393799,
      "learning_rate": 1.2664673115319043e-05,
      "loss": 0.7491,
      "step": 75600
    },
    {
      "epoch": 1.8633846153846154,
      "grad_norm": 8.606927871704102,
      "learning_rate": 1.2637322563293736e-05,
      "loss": 0.772,
      "step": 75700
    },
    {
      "epoch": 1.865846153846154,
      "grad_norm": 6.0007243156433105,
      "learning_rate": 1.2610245516788682e-05,
      "loss": 0.7385,
      "step": 75800
    },
    {
      "epoch": 1.8683076923076922,
      "grad_norm": 4.145765781402588,
      "learning_rate": 1.2582894964763373e-05,
      "loss": 0.7724,
      "step": 75900
    },
    {
      "epoch": 1.8707692307692307,
      "grad_norm": 7.264122486114502,
      "learning_rate": 1.2555544412738063e-05,
      "loss": 0.7748,
      "step": 76000
    },
    {
      "epoch": 1.8732307692307693,
      "grad_norm": 4.129087924957275,
      "learning_rate": 1.2528193860712755e-05,
      "loss": 0.7528,
      "step": 76100
    },
    {
      "epoch": 1.8756923076923075,
      "grad_norm": 4.310255527496338,
      "learning_rate": 1.2500843308687447e-05,
      "loss": 0.7891,
      "step": 76200
    },
    {
      "epoch": 1.8781538461538463,
      "grad_norm": 4.332014083862305,
      "learning_rate": 1.247349275666214e-05,
      "loss": 0.7532,
      "step": 76300
    },
    {
      "epoch": 1.8806153846153846,
      "grad_norm": 4.491020202636719,
      "learning_rate": 1.2446142204636832e-05,
      "loss": 0.7759,
      "step": 76400
    },
    {
      "epoch": 1.883076923076923,
      "grad_norm": 7.290433883666992,
      "learning_rate": 1.241879165261152e-05,
      "loss": 0.7455,
      "step": 76500
    },
    {
      "epoch": 1.8855384615384616,
      "grad_norm": 4.915802478790283,
      "learning_rate": 1.2391441100586213e-05,
      "loss": 0.7799,
      "step": 76600
    },
    {
      "epoch": 1.888,
      "grad_norm": 4.766322135925293,
      "learning_rate": 1.2364090548560905e-05,
      "loss": 0.7583,
      "step": 76700
    },
    {
      "epoch": 1.8904615384615384,
      "grad_norm": 8.546625137329102,
      "learning_rate": 1.2336739996535597e-05,
      "loss": 0.762,
      "step": 76800
    },
    {
      "epoch": 1.892923076923077,
      "grad_norm": 4.7419114112854,
      "learning_rate": 1.230938944451029e-05,
      "loss": 0.7477,
      "step": 76900
    },
    {
      "epoch": 1.8953846153846152,
      "grad_norm": 6.193477153778076,
      "learning_rate": 1.228203889248498e-05,
      "loss": 0.7835,
      "step": 77000
    },
    {
      "epoch": 1.897846153846154,
      "grad_norm": 4.148632526397705,
      "learning_rate": 1.2254688340459672e-05,
      "loss": 0.7571,
      "step": 77100
    },
    {
      "epoch": 1.9003076923076923,
      "grad_norm": 7.474436283111572,
      "learning_rate": 1.2227337788434363e-05,
      "loss": 0.7536,
      "step": 77200
    },
    {
      "epoch": 1.9027692307692308,
      "grad_norm": 5.788622856140137,
      "learning_rate": 1.2199987236409055e-05,
      "loss": 0.7692,
      "step": 77300
    },
    {
      "epoch": 1.9052307692307693,
      "grad_norm": 5.459519386291504,
      "learning_rate": 1.2172636684383747e-05,
      "loss": 0.7433,
      "step": 77400
    },
    {
      "epoch": 1.9076923076923076,
      "grad_norm": 5.136480808258057,
      "learning_rate": 1.2145286132358438e-05,
      "loss": 0.7591,
      "step": 77500
    },
    {
      "epoch": 1.9101538461538463,
      "grad_norm": 3.3275623321533203,
      "learning_rate": 1.211793558033313e-05,
      "loss": 0.7499,
      "step": 77600
    },
    {
      "epoch": 1.9126153846153846,
      "grad_norm": 3.992469310760498,
      "learning_rate": 1.2090585028307822e-05,
      "loss": 0.7551,
      "step": 77700
    },
    {
      "epoch": 1.9150769230769231,
      "grad_norm": 5.754732131958008,
      "learning_rate": 1.2063234476282513e-05,
      "loss": 0.7908,
      "step": 77800
    },
    {
      "epoch": 1.9175384615384616,
      "grad_norm": 7.426863670349121,
      "learning_rate": 1.2035883924257205e-05,
      "loss": 0.7498,
      "step": 77900
    },
    {
      "epoch": 1.92,
      "grad_norm": 4.7278151512146,
      "learning_rate": 1.200880687775215e-05,
      "loss": 0.7671,
      "step": 78000
    },
    {
      "epoch": 1.9224615384615384,
      "grad_norm": 5.45509672164917,
      "learning_rate": 1.1981456325726842e-05,
      "loss": 0.7606,
      "step": 78100
    },
    {
      "epoch": 1.924923076923077,
      "grad_norm": 6.200279712677002,
      "learning_rate": 1.1954105773701534e-05,
      "loss": 0.7377,
      "step": 78200
    },
    {
      "epoch": 1.9273846153846153,
      "grad_norm": 4.161108016967773,
      "learning_rate": 1.1926755221676224e-05,
      "loss": 0.7264,
      "step": 78300
    },
    {
      "epoch": 1.929846153846154,
      "grad_norm": 3.7728917598724365,
      "learning_rate": 1.1899404669650915e-05,
      "loss": 0.7706,
      "step": 78400
    },
    {
      "epoch": 1.9323076923076923,
      "grad_norm": 4.676133155822754,
      "learning_rate": 1.1872054117625607e-05,
      "loss": 0.7506,
      "step": 78500
    },
    {
      "epoch": 1.9347692307692308,
      "grad_norm": 4.191352844238281,
      "learning_rate": 1.18447035656003e-05,
      "loss": 0.7826,
      "step": 78600
    },
    {
      "epoch": 1.9372307692307693,
      "grad_norm": 4.677340030670166,
      "learning_rate": 1.1817353013574992e-05,
      "loss": 0.7423,
      "step": 78700
    },
    {
      "epoch": 1.9396923076923076,
      "grad_norm": 4.914600849151611,
      "learning_rate": 1.1790002461549684e-05,
      "loss": 0.7574,
      "step": 78800
    },
    {
      "epoch": 1.9421538461538461,
      "grad_norm": 6.190972328186035,
      "learning_rate": 1.1762651909524373e-05,
      "loss": 0.7581,
      "step": 78900
    },
    {
      "epoch": 1.9446153846153846,
      "grad_norm": 5.633558750152588,
      "learning_rate": 1.1735301357499065e-05,
      "loss": 0.7718,
      "step": 79000
    },
    {
      "epoch": 1.947076923076923,
      "grad_norm": 6.666930675506592,
      "learning_rate": 1.1707950805473757e-05,
      "loss": 0.7436,
      "step": 79100
    },
    {
      "epoch": 1.9495384615384617,
      "grad_norm": 5.2858500480651855,
      "learning_rate": 1.168060025344845e-05,
      "loss": 0.7236,
      "step": 79200
    },
    {
      "epoch": 1.952,
      "grad_norm": 4.153843879699707,
      "learning_rate": 1.1653249701423142e-05,
      "loss": 0.7692,
      "step": 79300
    },
    {
      "epoch": 1.9544615384615385,
      "grad_norm": 4.339346408843994,
      "learning_rate": 1.1625899149397832e-05,
      "loss": 0.7649,
      "step": 79400
    },
    {
      "epoch": 1.956923076923077,
      "grad_norm": 6.3988566398620605,
      "learning_rate": 1.1598548597372524e-05,
      "loss": 0.7138,
      "step": 79500
    },
    {
      "epoch": 1.9593846153846153,
      "grad_norm": 10.196331977844238,
      "learning_rate": 1.1571198045347215e-05,
      "loss": 0.757,
      "step": 79600
    },
    {
      "epoch": 1.9618461538461538,
      "grad_norm": 6.145411491394043,
      "learning_rate": 1.1543847493321907e-05,
      "loss": 0.7891,
      "step": 79700
    },
    {
      "epoch": 1.9643076923076923,
      "grad_norm": 4.440493583679199,
      "learning_rate": 1.15164969412966e-05,
      "loss": 0.7461,
      "step": 79800
    },
    {
      "epoch": 1.9667692307692306,
      "grad_norm": 4.16583776473999,
      "learning_rate": 1.1489146389271291e-05,
      "loss": 0.733,
      "step": 79900
    },
    {
      "epoch": 1.9692307692307693,
      "grad_norm": 4.4696526527404785,
      "learning_rate": 1.1461795837245982e-05,
      "loss": 0.7709,
      "step": 80000
    },
    {
      "epoch": 1.9716923076923076,
      "grad_norm": 5.09701681137085,
      "learning_rate": 1.1434445285220674e-05,
      "loss": 0.7325,
      "step": 80100
    },
    {
      "epoch": 1.9741538461538461,
      "grad_norm": 8.374302864074707,
      "learning_rate": 1.1407368238715619e-05,
      "loss": 0.7623,
      "step": 80200
    },
    {
      "epoch": 1.9766153846153847,
      "grad_norm": 4.170773506164551,
      "learning_rate": 1.138001768669031e-05,
      "loss": 0.7539,
      "step": 80300
    },
    {
      "epoch": 1.979076923076923,
      "grad_norm": 9.633661270141602,
      "learning_rate": 1.1352667134665001e-05,
      "loss": 0.7649,
      "step": 80400
    },
    {
      "epoch": 1.9815384615384617,
      "grad_norm": 13.844989776611328,
      "learning_rate": 1.1325316582639694e-05,
      "loss": 0.7209,
      "step": 80500
    },
    {
      "epoch": 1.984,
      "grad_norm": 6.589996337890625,
      "learning_rate": 1.1297966030614386e-05,
      "loss": 0.7856,
      "step": 80600
    },
    {
      "epoch": 1.9864615384615385,
      "grad_norm": 6.842148780822754,
      "learning_rate": 1.1270615478589076e-05,
      "loss": 0.7442,
      "step": 80700
    },
    {
      "epoch": 1.988923076923077,
      "grad_norm": 5.386075019836426,
      "learning_rate": 1.1243264926563767e-05,
      "loss": 0.7393,
      "step": 80800
    },
    {
      "epoch": 1.9913846153846153,
      "grad_norm": 6.424856185913086,
      "learning_rate": 1.121591437453846e-05,
      "loss": 0.7765,
      "step": 80900
    },
    {
      "epoch": 1.9938461538461538,
      "grad_norm": 5.6351752281188965,
      "learning_rate": 1.1188563822513151e-05,
      "loss": 0.7309,
      "step": 81000
    },
    {
      "epoch": 1.9963076923076923,
      "grad_norm": 5.7567524909973145,
      "learning_rate": 1.1161213270487844e-05,
      "loss": 0.7228,
      "step": 81100
    },
    {
      "epoch": 1.9987692307692306,
      "grad_norm": 7.330635070800781,
      "learning_rate": 1.1133862718462536e-05,
      "loss": 0.7156,
      "step": 81200
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.65212,
      "eval_f1": 0.6526849204262344,
      "eval_loss": 0.800343930721283,
      "eval_precision": 0.653511885400778,
      "eval_recall": 0.65212,
      "eval_runtime": 10.2675,
      "eval_samples_per_second": 4869.734,
      "eval_steps_per_second": 304.358,
      "step": 81250
    }
  ],
  "logging_steps": 100,
  "max_steps": 121875,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "EarlyStoppingCallback": {
      "args": {
        "early_stopping_patience": 4,
        "early_stopping_threshold": 0.0
      },
      "attributes": {
        "early_stopping_patience_counter": 0
      }
    },
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 4.3054207872e+16,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
