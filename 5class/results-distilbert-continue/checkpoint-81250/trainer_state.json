{
  "best_metric": 0.9335671067237854,
  "best_model_checkpoint": "./5class/results-distilbert-continue/checkpoint-81250",
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 81250,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.012307692307692308,
      "grad_norm": 6.235907554626465,
      "learning_rate": 4.913109831142619e-07,
      "loss": 0.7194,
      "step": 500
    },
    {
      "epoch": 0.024615384615384615,
      "grad_norm": 5.420589923858643,
      "learning_rate": 9.836065573770493e-07,
      "loss": 0.7238,
      "step": 1000
    },
    {
      "epoch": 0.036923076923076927,
      "grad_norm": 4.386971950531006,
      "learning_rate": 1.4759021316398368e-06,
      "loss": 0.7073,
      "step": 1500
    },
    {
      "epoch": 0.04923076923076923,
      "grad_norm": 10.253539085388184,
      "learning_rate": 1.9681977059026242e-06,
      "loss": 0.704,
      "step": 2000
    },
    {
      "epoch": 0.06153846153846154,
      "grad_norm": 7.7263054847717285,
      "learning_rate": 2.459508689016886e-06,
      "loss": 0.7024,
      "step": 2500
    },
    {
      "epoch": 0.07384615384615385,
      "grad_norm": 4.534252643585205,
      "learning_rate": 2.9518042632796735e-06,
      "loss": 0.6856,
      "step": 3000
    },
    {
      "epoch": 0.08615384615384615,
      "grad_norm": 7.772571086883545,
      "learning_rate": 3.4431152463939353e-06,
      "loss": 0.6931,
      "step": 3500
    },
    {
      "epoch": 0.09846153846153846,
      "grad_norm": 6.030098915100098,
      "learning_rate": 3.935410820656723e-06,
      "loss": 0.6904,
      "step": 4000
    },
    {
      "epoch": 0.11076923076923077,
      "grad_norm": 5.330897808074951,
      "learning_rate": 4.427706394919509e-06,
      "loss": 0.703,
      "step": 4500
    },
    {
      "epoch": 0.12307692307692308,
      "grad_norm": 8.592127799987793,
      "learning_rate": 4.920001969182298e-06,
      "loss": 0.6811,
      "step": 5000
    },
    {
      "epoch": 0.13538461538461538,
      "grad_norm": 9.442900657653809,
      "learning_rate": 5.412297543445085e-06,
      "loss": 0.6813,
      "step": 5500
    },
    {
      "epoch": 0.1476923076923077,
      "grad_norm": 5.493335723876953,
      "learning_rate": 5.904593117707872e-06,
      "loss": 0.6845,
      "step": 6000
    },
    {
      "epoch": 0.16,
      "grad_norm": 6.05517053604126,
      "learning_rate": 6.3959041008221345e-06,
      "loss": 0.6631,
      "step": 6500
    },
    {
      "epoch": 0.1723076923076923,
      "grad_norm": 7.111871719360352,
      "learning_rate": 6.888199675084921e-06,
      "loss": 0.6922,
      "step": 7000
    },
    {
      "epoch": 0.18461538461538463,
      "grad_norm": 10.681289672851562,
      "learning_rate": 7.380495249347709e-06,
      "loss": 0.6746,
      "step": 7500
    },
    {
      "epoch": 0.19692307692307692,
      "grad_norm": 4.030351638793945,
      "learning_rate": 7.872790823610497e-06,
      "loss": 0.6602,
      "step": 8000
    },
    {
      "epoch": 0.20923076923076922,
      "grad_norm": 8.314674377441406,
      "learning_rate": 8.365086397873284e-06,
      "loss": 0.6767,
      "step": 8500
    },
    {
      "epoch": 0.22153846153846155,
      "grad_norm": 5.552408218383789,
      "learning_rate": 8.85738197213607e-06,
      "loss": 0.6637,
      "step": 9000
    },
    {
      "epoch": 0.23384615384615384,
      "grad_norm": 7.067431449890137,
      "learning_rate": 9.349677546398859e-06,
      "loss": 0.6702,
      "step": 9500
    },
    {
      "epoch": 0.24615384615384617,
      "grad_norm": 9.03625774383545,
      "learning_rate": 9.841973120661647e-06,
      "loss": 0.6504,
      "step": 10000
    },
    {
      "epoch": 0.25846153846153846,
      "grad_norm": 9.089900016784668,
      "learning_rate": 1.0334268694924434e-05,
      "loss": 0.6582,
      "step": 10500
    },
    {
      "epoch": 0.27076923076923076,
      "grad_norm": 4.472917079925537,
      "learning_rate": 1.0825579678038696e-05,
      "loss": 0.6497,
      "step": 11000
    },
    {
      "epoch": 0.28307692307692306,
      "grad_norm": 7.13369607925415,
      "learning_rate": 1.1317875252301483e-05,
      "loss": 0.6547,
      "step": 11500
    },
    {
      "epoch": 0.2953846153846154,
      "grad_norm": 7.278608322143555,
      "learning_rate": 1.1810170826564271e-05,
      "loss": 0.6435,
      "step": 12000
    },
    {
      "epoch": 0.3076923076923077,
      "grad_norm": 8.962124824523926,
      "learning_rate": 1.2301481809678532e-05,
      "loss": 0.6367,
      "step": 12500
    },
    {
      "epoch": 0.32,
      "grad_norm": 8.136103630065918,
      "learning_rate": 1.2793777383941319e-05,
      "loss": 0.6405,
      "step": 13000
    },
    {
      "epoch": 0.3323076923076923,
      "grad_norm": 6.8364081382751465,
      "learning_rate": 1.3286072958204105e-05,
      "loss": 0.6392,
      "step": 13500
    },
    {
      "epoch": 0.3446153846153846,
      "grad_norm": 10.370936393737793,
      "learning_rate": 1.3778368532466895e-05,
      "loss": 0.6254,
      "step": 14000
    },
    {
      "epoch": 0.3569230769230769,
      "grad_norm": 7.019042015075684,
      "learning_rate": 1.4270664106729682e-05,
      "loss": 0.6547,
      "step": 14500
    },
    {
      "epoch": 0.36923076923076925,
      "grad_norm": 7.6631903648376465,
      "learning_rate": 1.4762959680992469e-05,
      "loss": 0.6409,
      "step": 15000
    },
    {
      "epoch": 0.38153846153846155,
      "grad_norm": 5.795406818389893,
      "learning_rate": 1.5255255255255257e-05,
      "loss": 0.6256,
      "step": 15500
    },
    {
      "epoch": 0.39384615384615385,
      "grad_norm": 5.253594398498535,
      "learning_rate": 1.5747550829518042e-05,
      "loss": 0.6419,
      "step": 16000
    },
    {
      "epoch": 0.40615384615384614,
      "grad_norm": 9.296622276306152,
      "learning_rate": 1.6238861812632305e-05,
      "loss": 0.6289,
      "step": 16500
    },
    {
      "epoch": 0.41846153846153844,
      "grad_norm": 5.5691986083984375,
      "learning_rate": 1.6730172795746567e-05,
      "loss": 0.6187,
      "step": 17000
    },
    {
      "epoch": 0.4307692307692308,
      "grad_norm": 9.330429077148438,
      "learning_rate": 1.7222468370009354e-05,
      "loss": 0.6147,
      "step": 17500
    },
    {
      "epoch": 0.4430769230769231,
      "grad_norm": 9.765186309814453,
      "learning_rate": 1.771476394427214e-05,
      "loss": 0.6249,
      "step": 18000
    },
    {
      "epoch": 0.4553846153846154,
      "grad_norm": 4.297309398651123,
      "learning_rate": 1.820705951853493e-05,
      "loss": 0.6336,
      "step": 18500
    },
    {
      "epoch": 0.4676923076923077,
      "grad_norm": 16.720396041870117,
      "learning_rate": 1.8699355092797717e-05,
      "loss": 0.6258,
      "step": 19000
    },
    {
      "epoch": 0.48,
      "grad_norm": 7.8436737060546875,
      "learning_rate": 1.9191650667060504e-05,
      "loss": 0.6153,
      "step": 19500
    },
    {
      "epoch": 0.49230769230769234,
      "grad_norm": 11.690386772155762,
      "learning_rate": 1.9683946241323294e-05,
      "loss": 0.6182,
      "step": 20000
    },
    {
      "epoch": 0.5046153846153846,
      "grad_norm": 13.126270294189453,
      "learning_rate": 1.9980526442465486e-05,
      "loss": 0.6239,
      "step": 20500
    },
    {
      "epoch": 0.5169230769230769,
      "grad_norm": 8.391670227050781,
      "learning_rate": 1.9925825438155046e-05,
      "loss": 0.6231,
      "step": 21000
    },
    {
      "epoch": 0.5292307692307693,
      "grad_norm": 14.8555908203125,
      "learning_rate": 1.9871124433844606e-05,
      "loss": 0.6275,
      "step": 21500
    },
    {
      "epoch": 0.5415384615384615,
      "grad_norm": 11.088796615600586,
      "learning_rate": 1.9816423429534167e-05,
      "loss": 0.6129,
      "step": 22000
    },
    {
      "epoch": 0.5538461538461539,
      "grad_norm": 9.467132568359375,
      "learning_rate": 1.976183182723235e-05,
      "loss": 0.6305,
      "step": 22500
    },
    {
      "epoch": 0.5661538461538461,
      "grad_norm": 6.731119632720947,
      "learning_rate": 1.970713082292191e-05,
      "loss": 0.6267,
      "step": 23000
    },
    {
      "epoch": 0.5784615384615385,
      "grad_norm": 9.448387145996094,
      "learning_rate": 1.9652429818611473e-05,
      "loss": 0.6249,
      "step": 23500
    },
    {
      "epoch": 0.5907692307692308,
      "grad_norm": 7.409879684448242,
      "learning_rate": 1.959772881430103e-05,
      "loss": 0.6133,
      "step": 24000
    },
    {
      "epoch": 0.6030769230769231,
      "grad_norm": 11.546497344970703,
      "learning_rate": 1.9543027809990593e-05,
      "loss": 0.6256,
      "step": 24500
    },
    {
      "epoch": 0.6153846153846154,
      "grad_norm": 8.33176326751709,
      "learning_rate": 1.9488436207688775e-05,
      "loss": 0.6273,
      "step": 25000
    },
    {
      "epoch": 0.6276923076923077,
      "grad_norm": 11.668367385864258,
      "learning_rate": 1.9433735203378336e-05,
      "loss": 0.6287,
      "step": 25500
    },
    {
      "epoch": 0.64,
      "grad_norm": 5.472836494445801,
      "learning_rate": 1.9379034199067896e-05,
      "loss": 0.6344,
      "step": 26000
    },
    {
      "epoch": 0.6523076923076923,
      "grad_norm": 10.902223587036133,
      "learning_rate": 1.9324333194757456e-05,
      "loss": 0.6132,
      "step": 26500
    },
    {
      "epoch": 0.6646153846153846,
      "grad_norm": 7.5626349449157715,
      "learning_rate": 1.9269741592455638e-05,
      "loss": 0.6069,
      "step": 27000
    },
    {
      "epoch": 0.676923076923077,
      "grad_norm": 15.117897987365723,
      "learning_rate": 1.92150405881452e-05,
      "loss": 0.6121,
      "step": 27500
    },
    {
      "epoch": 0.6892307692307692,
      "grad_norm": 9.518050193786621,
      "learning_rate": 1.9160339583834762e-05,
      "loss": 0.625,
      "step": 28000
    },
    {
      "epoch": 0.7015384615384616,
      "grad_norm": 5.105159759521484,
      "learning_rate": 1.9105638579524323e-05,
      "loss": 0.6187,
      "step": 28500
    },
    {
      "epoch": 0.7138461538461538,
      "grad_norm": 8.951798439025879,
      "learning_rate": 1.90510469772225e-05,
      "loss": 0.6226,
      "step": 29000
    },
    {
      "epoch": 0.7261538461538461,
      "grad_norm": 6.9798102378845215,
      "learning_rate": 1.8996345972912065e-05,
      "loss": 0.6167,
      "step": 29500
    },
    {
      "epoch": 0.7384615384615385,
      "grad_norm": 7.351022720336914,
      "learning_rate": 1.8941644968601625e-05,
      "loss": 0.6097,
      "step": 30000
    },
    {
      "epoch": 0.7507692307692307,
      "grad_norm": 9.844980239868164,
      "learning_rate": 1.8886943964291186e-05,
      "loss": 0.623,
      "step": 30500
    },
    {
      "epoch": 0.7630769230769231,
      "grad_norm": 5.720944404602051,
      "learning_rate": 1.8832242959980746e-05,
      "loss": 0.6113,
      "step": 31000
    },
    {
      "epoch": 0.7753846153846153,
      "grad_norm": 6.653748512268066,
      "learning_rate": 1.8777651357678928e-05,
      "loss": 0.6066,
      "step": 31500
    },
    {
      "epoch": 0.7876923076923077,
      "grad_norm": 9.603177070617676,
      "learning_rate": 1.872295035336849e-05,
      "loss": 0.5933,
      "step": 32000
    },
    {
      "epoch": 0.8,
      "grad_norm": 7.581097602844238,
      "learning_rate": 1.866824934905805e-05,
      "loss": 0.6032,
      "step": 32500
    },
    {
      "epoch": 0.8123076923076923,
      "grad_norm": 9.322403907775879,
      "learning_rate": 1.8613548344747612e-05,
      "loss": 0.6049,
      "step": 33000
    },
    {
      "epoch": 0.8246153846153846,
      "grad_norm": 12.231279373168945,
      "learning_rate": 1.8558956742445794e-05,
      "loss": 0.6137,
      "step": 33500
    },
    {
      "epoch": 0.8369230769230769,
      "grad_norm": 7.718378067016602,
      "learning_rate": 1.8504255738135354e-05,
      "loss": 0.6034,
      "step": 34000
    },
    {
      "epoch": 0.8492307692307692,
      "grad_norm": 9.631194114685059,
      "learning_rate": 1.8449554733824915e-05,
      "loss": 0.6128,
      "step": 34500
    },
    {
      "epoch": 0.8615384615384616,
      "grad_norm": 9.993206024169922,
      "learning_rate": 1.8394853729514475e-05,
      "loss": 0.6013,
      "step": 35000
    },
    {
      "epoch": 0.8738461538461538,
      "grad_norm": 15.252096176147461,
      "learning_rate": 1.8340262127212657e-05,
      "loss": 0.6058,
      "step": 35500
    },
    {
      "epoch": 0.8861538461538462,
      "grad_norm": 9.963449478149414,
      "learning_rate": 1.8285561122902217e-05,
      "loss": 0.6099,
      "step": 36000
    },
    {
      "epoch": 0.8984615384615384,
      "grad_norm": 10.217989921569824,
      "learning_rate": 1.8230860118591778e-05,
      "loss": 0.6033,
      "step": 36500
    },
    {
      "epoch": 0.9107692307692308,
      "grad_norm": 9.018006324768066,
      "learning_rate": 1.817615911428134e-05,
      "loss": 0.6039,
      "step": 37000
    },
    {
      "epoch": 0.9230769230769231,
      "grad_norm": 14.037564277648926,
      "learning_rate": 1.81214581099709e-05,
      "loss": 0.5963,
      "step": 37500
    },
    {
      "epoch": 0.9353846153846154,
      "grad_norm": 11.439000129699707,
      "learning_rate": 1.8066866507669084e-05,
      "loss": 0.5873,
      "step": 38000
    },
    {
      "epoch": 0.9476923076923077,
      "grad_norm": 5.832624912261963,
      "learning_rate": 1.8012165503358644e-05,
      "loss": 0.6087,
      "step": 38500
    },
    {
      "epoch": 0.96,
      "grad_norm": 11.178743362426758,
      "learning_rate": 1.7957464499048204e-05,
      "loss": 0.6058,
      "step": 39000
    },
    {
      "epoch": 0.9723076923076923,
      "grad_norm": 10.329743385314941,
      "learning_rate": 1.7902763494737765e-05,
      "loss": 0.6087,
      "step": 39500
    },
    {
      "epoch": 0.9846153846153847,
      "grad_norm": 9.946330070495605,
      "learning_rate": 1.7848171892435946e-05,
      "loss": 0.6009,
      "step": 40000
    },
    {
      "epoch": 0.9969230769230769,
      "grad_norm": 10.605135917663574,
      "learning_rate": 1.7793580290134128e-05,
      "loss": 0.6009,
      "step": 40500
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.6402,
      "eval_f1": 0.6412990774388377,
      "eval_loss": 0.9697356820106506,
      "eval_precision": 0.6432442637509153,
      "eval_recall": 0.6402,
      "eval_runtime": 10.3088,
      "eval_samples_per_second": 4850.23,
      "eval_steps_per_second": 303.139,
      "step": 40625
    },
    {
      "epoch": 1.0092307692307692,
      "grad_norm": 10.627145767211914,
      "learning_rate": 1.773887928582369e-05,
      "loss": 0.5502,
      "step": 41000
    },
    {
      "epoch": 1.0215384615384615,
      "grad_norm": 7.767749786376953,
      "learning_rate": 1.768417828151325e-05,
      "loss": 0.5255,
      "step": 41500
    },
    {
      "epoch": 1.0338461538461539,
      "grad_norm": 17.375732421875,
      "learning_rate": 1.7629477277202813e-05,
      "loss": 0.5175,
      "step": 42000
    },
    {
      "epoch": 1.0461538461538462,
      "grad_norm": 12.502488136291504,
      "learning_rate": 1.757477627289237e-05,
      "loss": 0.5187,
      "step": 42500
    },
    {
      "epoch": 1.0584615384615386,
      "grad_norm": 11.442460060119629,
      "learning_rate": 1.7520075268581933e-05,
      "loss": 0.5139,
      "step": 43000
    },
    {
      "epoch": 1.0707692307692307,
      "grad_norm": 12.438642501831055,
      "learning_rate": 1.7465374264271494e-05,
      "loss": 0.5169,
      "step": 43500
    },
    {
      "epoch": 1.083076923076923,
      "grad_norm": 29.349145889282227,
      "learning_rate": 1.7410782661969676e-05,
      "loss": 0.5217,
      "step": 44000
    },
    {
      "epoch": 1.0953846153846154,
      "grad_norm": 9.629353523254395,
      "learning_rate": 1.7356081657659236e-05,
      "loss": 0.5143,
      "step": 44500
    },
    {
      "epoch": 1.1076923076923078,
      "grad_norm": 9.794981956481934,
      "learning_rate": 1.7301380653348796e-05,
      "loss": 0.5271,
      "step": 45000
    },
    {
      "epoch": 1.12,
      "grad_norm": 10.525496482849121,
      "learning_rate": 1.7246679649038357e-05,
      "loss": 0.532,
      "step": 45500
    },
    {
      "epoch": 1.1323076923076922,
      "grad_norm": 18.645837783813477,
      "learning_rate": 1.7191978644727917e-05,
      "loss": 0.5112,
      "step": 46000
    },
    {
      "epoch": 1.1446153846153846,
      "grad_norm": 9.056897163391113,
      "learning_rate": 1.7137387042426102e-05,
      "loss": 0.5122,
      "step": 46500
    },
    {
      "epoch": 1.156923076923077,
      "grad_norm": 11.752042770385742,
      "learning_rate": 1.7082686038115663e-05,
      "loss": 0.5217,
      "step": 47000
    },
    {
      "epoch": 1.1692307692307693,
      "grad_norm": 15.205658912658691,
      "learning_rate": 1.702798503380522e-05,
      "loss": 0.5391,
      "step": 47500
    },
    {
      "epoch": 1.1815384615384614,
      "grad_norm": 15.34931755065918,
      "learning_rate": 1.6973284029494783e-05,
      "loss": 0.516,
      "step": 48000
    },
    {
      "epoch": 1.1938461538461538,
      "grad_norm": 13.071495056152344,
      "learning_rate": 1.6918583025184344e-05,
      "loss": 0.5232,
      "step": 48500
    },
    {
      "epoch": 1.2061538461538461,
      "grad_norm": 15.15710735321045,
      "learning_rate": 1.6863882020873904e-05,
      "loss": 0.513,
      "step": 49000
    },
    {
      "epoch": 1.2184615384615385,
      "grad_norm": 13.68754768371582,
      "learning_rate": 1.6809181016563465e-05,
      "loss": 0.5352,
      "step": 49500
    },
    {
      "epoch": 1.2307692307692308,
      "grad_norm": 15.999058723449707,
      "learning_rate": 1.675448001225303e-05,
      "loss": 0.5138,
      "step": 50000
    },
    {
      "epoch": 1.2430769230769232,
      "grad_norm": 22.102787017822266,
      "learning_rate": 1.6699779007942585e-05,
      "loss": 0.5366,
      "step": 50500
    },
    {
      "epoch": 1.2553846153846153,
      "grad_norm": 10.255233764648438,
      "learning_rate": 1.6645187405640767e-05,
      "loss": 0.5454,
      "step": 51000
    },
    {
      "epoch": 1.2676923076923077,
      "grad_norm": 7.330542087554932,
      "learning_rate": 1.659048640133033e-05,
      "loss": 0.5367,
      "step": 51500
    },
    {
      "epoch": 1.28,
      "grad_norm": 11.206849098205566,
      "learning_rate": 1.653578539701989e-05,
      "loss": 0.5549,
      "step": 52000
    },
    {
      "epoch": 1.2923076923076924,
      "grad_norm": 18.56911277770996,
      "learning_rate": 1.6481193794718073e-05,
      "loss": 0.5295,
      "step": 52500
    },
    {
      "epoch": 1.3046153846153845,
      "grad_norm": 13.22836685180664,
      "learning_rate": 1.6426492790407633e-05,
      "loss": 0.5346,
      "step": 53000
    },
    {
      "epoch": 1.3169230769230769,
      "grad_norm": 8.97146987915039,
      "learning_rate": 1.6371791786097194e-05,
      "loss": 0.5213,
      "step": 53500
    },
    {
      "epoch": 1.3292307692307692,
      "grad_norm": 14.862527847290039,
      "learning_rate": 1.6317090781786754e-05,
      "loss": 0.5475,
      "step": 54000
    },
    {
      "epoch": 1.3415384615384616,
      "grad_norm": 18.482351303100586,
      "learning_rate": 1.6262389777476315e-05,
      "loss": 0.5399,
      "step": 54500
    },
    {
      "epoch": 1.353846153846154,
      "grad_norm": 10.629386901855469,
      "learning_rate": 1.62077981751745e-05,
      "loss": 0.5258,
      "step": 55000
    },
    {
      "epoch": 1.3661538461538463,
      "grad_norm": 14.164313316345215,
      "learning_rate": 1.6153097170864057e-05,
      "loss": 0.5545,
      "step": 55500
    },
    {
      "epoch": 1.3784615384615384,
      "grad_norm": 8.464454650878906,
      "learning_rate": 1.6098396166553617e-05,
      "loss": 0.5186,
      "step": 56000
    },
    {
      "epoch": 1.3907692307692308,
      "grad_norm": 11.089134216308594,
      "learning_rate": 1.604369516224318e-05,
      "loss": 0.534,
      "step": 56500
    },
    {
      "epoch": 1.403076923076923,
      "grad_norm": 13.926067352294922,
      "learning_rate": 1.598899415793274e-05,
      "loss": 0.5448,
      "step": 57000
    },
    {
      "epoch": 1.4153846153846155,
      "grad_norm": 15.20297622680664,
      "learning_rate": 1.59342931536223e-05,
      "loss": 0.5376,
      "step": 57500
    },
    {
      "epoch": 1.4276923076923076,
      "grad_norm": 15.865894317626953,
      "learning_rate": 1.5879592149311862e-05,
      "loss": 0.5259,
      "step": 58000
    },
    {
      "epoch": 1.44,
      "grad_norm": 4.878961563110352,
      "learning_rate": 1.5824891145001426e-05,
      "loss": 0.5516,
      "step": 58500
    },
    {
      "epoch": 1.4523076923076923,
      "grad_norm": 10.885297775268555,
      "learning_rate": 1.5770299542699604e-05,
      "loss": 0.523,
      "step": 59000
    },
    {
      "epoch": 1.4646153846153847,
      "grad_norm": 15.223341941833496,
      "learning_rate": 1.5715598538389165e-05,
      "loss": 0.5501,
      "step": 59500
    },
    {
      "epoch": 1.476923076923077,
      "grad_norm": 17.385122299194336,
      "learning_rate": 1.566089753407873e-05,
      "loss": 0.5682,
      "step": 60000
    },
    {
      "epoch": 1.4892307692307694,
      "grad_norm": 12.694388389587402,
      "learning_rate": 1.5606305931776907e-05,
      "loss": 0.5494,
      "step": 60500
    },
    {
      "epoch": 1.5015384615384615,
      "grad_norm": 14.49990463256836,
      "learning_rate": 1.555160492746647e-05,
      "loss": 0.5467,
      "step": 61000
    },
    {
      "epoch": 1.5138461538461538,
      "grad_norm": 12.225484848022461,
      "learning_rate": 1.549690392315603e-05,
      "loss": 0.5526,
      "step": 61500
    },
    {
      "epoch": 1.5261538461538462,
      "grad_norm": 15.648157119750977,
      "learning_rate": 1.544220291884559e-05,
      "loss": 0.5424,
      "step": 62000
    },
    {
      "epoch": 1.5384615384615383,
      "grad_norm": 11.839566230773926,
      "learning_rate": 1.538750191453515e-05,
      "loss": 0.5539,
      "step": 62500
    },
    {
      "epoch": 1.5507692307692307,
      "grad_norm": 9.24014663696289,
      "learning_rate": 1.5332800910224712e-05,
      "loss": 0.5513,
      "step": 63000
    },
    {
      "epoch": 1.563076923076923,
      "grad_norm": 12.026843070983887,
      "learning_rate": 1.5278099905914276e-05,
      "loss": 0.5463,
      "step": 63500
    },
    {
      "epoch": 1.5753846153846154,
      "grad_norm": 11.90099048614502,
      "learning_rate": 1.5223398901603834e-05,
      "loss": 0.5522,
      "step": 64000
    },
    {
      "epoch": 1.5876923076923077,
      "grad_norm": 12.263263702392578,
      "learning_rate": 1.5168807299302018e-05,
      "loss": 0.5504,
      "step": 64500
    },
    {
      "epoch": 1.6,
      "grad_norm": 6.061073303222656,
      "learning_rate": 1.5114106294991577e-05,
      "loss": 0.5529,
      "step": 65000
    },
    {
      "epoch": 1.6123076923076924,
      "grad_norm": 10.41832447052002,
      "learning_rate": 1.5059405290681137e-05,
      "loss": 0.5565,
      "step": 65500
    },
    {
      "epoch": 1.6246153846153846,
      "grad_norm": 12.060627937316895,
      "learning_rate": 1.5004704286370699e-05,
      "loss": 0.5662,
      "step": 66000
    },
    {
      "epoch": 1.636923076923077,
      "grad_norm": 14.420104026794434,
      "learning_rate": 1.495000328206026e-05,
      "loss": 0.5482,
      "step": 66500
    },
    {
      "epoch": 1.6492307692307693,
      "grad_norm": 17.345855712890625,
      "learning_rate": 1.4895411679758443e-05,
      "loss": 0.5505,
      "step": 67000
    },
    {
      "epoch": 1.6615384615384614,
      "grad_norm": 12.556368827819824,
      "learning_rate": 1.4840710675448002e-05,
      "loss": 0.5685,
      "step": 67500
    },
    {
      "epoch": 1.6738461538461538,
      "grad_norm": 15.963294982910156,
      "learning_rate": 1.4786009671137562e-05,
      "loss": 0.5743,
      "step": 68000
    },
    {
      "epoch": 1.6861538461538461,
      "grad_norm": 14.80000114440918,
      "learning_rate": 1.4731308666827124e-05,
      "loss": 0.5598,
      "step": 68500
    },
    {
      "epoch": 1.6984615384615385,
      "grad_norm": 14.89709758758545,
      "learning_rate": 1.4676717064525306e-05,
      "loss": 0.5705,
      "step": 69000
    },
    {
      "epoch": 1.7107692307692308,
      "grad_norm": 21.09687042236328,
      "learning_rate": 1.4622016060214868e-05,
      "loss": 0.5554,
      "step": 69500
    },
    {
      "epoch": 1.7230769230769232,
      "grad_norm": 8.40927505493164,
      "learning_rate": 1.4567424457913048e-05,
      "loss": 0.5638,
      "step": 70000
    },
    {
      "epoch": 1.7353846153846155,
      "grad_norm": 17.958415985107422,
      "learning_rate": 1.4512723453602608e-05,
      "loss": 0.5782,
      "step": 70500
    },
    {
      "epoch": 1.7476923076923077,
      "grad_norm": 15.908892631530762,
      "learning_rate": 1.445802244929217e-05,
      "loss": 0.5665,
      "step": 71000
    },
    {
      "epoch": 1.76,
      "grad_norm": 13.685245513916016,
      "learning_rate": 1.440332144498173e-05,
      "loss": 0.5741,
      "step": 71500
    },
    {
      "epoch": 1.7723076923076924,
      "grad_norm": 18.964033126831055,
      "learning_rate": 1.4348729842679914e-05,
      "loss": 0.5682,
      "step": 72000
    },
    {
      "epoch": 1.7846153846153845,
      "grad_norm": 16.249977111816406,
      "learning_rate": 1.4294028838369473e-05,
      "loss": 0.5516,
      "step": 72500
    },
    {
      "epoch": 1.7969230769230768,
      "grad_norm": 11.064038276672363,
      "learning_rate": 1.4239327834059033e-05,
      "loss": 0.5904,
      "step": 73000
    },
    {
      "epoch": 1.8092307692307692,
      "grad_norm": 11.2637300491333,
      "learning_rate": 1.4184626829748595e-05,
      "loss": 0.5794,
      "step": 73500
    },
    {
      "epoch": 1.8215384615384616,
      "grad_norm": 10.865972518920898,
      "learning_rate": 1.4129925825438156e-05,
      "loss": 0.5852,
      "step": 74000
    },
    {
      "epoch": 1.833846153846154,
      "grad_norm": 7.7823591232299805,
      "learning_rate": 1.4075224821127718e-05,
      "loss": 0.555,
      "step": 74500
    },
    {
      "epoch": 1.8461538461538463,
      "grad_norm": 14.328938484191895,
      "learning_rate": 1.4020523816817278e-05,
      "loss": 0.566,
      "step": 75000
    },
    {
      "epoch": 1.8584615384615386,
      "grad_norm": 16.925867080688477,
      "learning_rate": 1.396582281250684e-05,
      "loss": 0.5795,
      "step": 75500
    },
    {
      "epoch": 1.8707692307692307,
      "grad_norm": 14.345024108886719,
      "learning_rate": 1.391123121020502e-05,
      "loss": 0.5822,
      "step": 76000
    },
    {
      "epoch": 1.883076923076923,
      "grad_norm": 9.721428871154785,
      "learning_rate": 1.385653020589458e-05,
      "loss": 0.5787,
      "step": 76500
    },
    {
      "epoch": 1.8953846153846152,
      "grad_norm": 11.019097328186035,
      "learning_rate": 1.3801829201584143e-05,
      "loss": 0.5937,
      "step": 77000
    },
    {
      "epoch": 1.9076923076923076,
      "grad_norm": 8.881879806518555,
      "learning_rate": 1.3747128197273703e-05,
      "loss": 0.5774,
      "step": 77500
    },
    {
      "epoch": 1.92,
      "grad_norm": 11.100066184997559,
      "learning_rate": 1.3692536594971887e-05,
      "loss": 0.5855,
      "step": 78000
    },
    {
      "epoch": 1.9323076923076923,
      "grad_norm": 10.622444152832031,
      "learning_rate": 1.3637835590661445e-05,
      "loss": 0.5786,
      "step": 78500
    },
    {
      "epoch": 1.9446153846153846,
      "grad_norm": 13.51728343963623,
      "learning_rate": 1.3583134586351006e-05,
      "loss": 0.5821,
      "step": 79000
    },
    {
      "epoch": 1.956923076923077,
      "grad_norm": 24.607501983642578,
      "learning_rate": 1.3528433582040568e-05,
      "loss": 0.5728,
      "step": 79500
    },
    {
      "epoch": 1.9692307692307693,
      "grad_norm": 9.202476501464844,
      "learning_rate": 1.3473732577730128e-05,
      "loss": 0.5937,
      "step": 80000
    },
    {
      "epoch": 1.9815384615384617,
      "grad_norm": 22.724943161010742,
      "learning_rate": 1.341903157341969e-05,
      "loss": 0.5768,
      "step": 80500
    },
    {
      "epoch": 1.9938461538461538,
      "grad_norm": 8.09631633758545,
      "learning_rate": 1.3364330569109249e-05,
      "loss": 0.5935,
      "step": 81000
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.63706,
      "eval_f1": 0.6384630073861929,
      "eval_loss": 0.9335671067237854,
      "eval_precision": 0.6403266454011062,
      "eval_recall": 0.63706,
      "eval_runtime": 10.2574,
      "eval_samples_per_second": 4874.519,
      "eval_steps_per_second": 304.657,
      "step": 81250
    }
  ],
  "logging_steps": 500,
  "max_steps": 203125,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 4.3054207872e+16,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
